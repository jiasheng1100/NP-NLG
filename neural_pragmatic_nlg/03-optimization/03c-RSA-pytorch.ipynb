{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqHxDfXciwpo"
      },
      "source": [
        "Sheet 3.2: Optimizing an RSA model\n",
        "==================================\n",
        "\n",
        "**Author:** Michael Franke\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auIusaE3iwpp"
      },
      "source": [
        "Here we will explore how to use PyTorch to find optimized values for the parameters of a vanilla RSA model for reference games.\n",
        "This serves several purposes: (i) it provides a chance to exercise with the basics of parameter optimization in PyTorch; and (ii) we learn to think about models as objects that can (and must!) be critically tested with respect to their predictive ability.\n",
        "\n",
        "To fit a vanilla RSA model, we use data from [Qing & Franke (2016)](https://michael-franke.github.io/heimseite/Papers/QingFranke_2013_Variations_on_Bayes.pdf). A Bayesian data analysis for this data set and model set up is provided in [this chapter of problang.org](http://www.problang.org/chapters/app-04-BDA.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student: Jia Sheng (5371477)"
      ],
      "metadata": {
        "id": "EbeIx1xPiyrq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uek0pGJyiwpr"
      },
      "source": [
        "## Packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Wmfg7Qiwps"
      },
      "source": [
        "We will need to import the \\`torch\\` package for the main functionality.\n",
        "In order to have a convenient handle, we load the \\`torch.nn.functional\\` package into variable \\`F\\`.\n",
        "We use this to refer to the normalization function for tensors: \\`F.normalize\\`.\n",
        "We use the \\`warnings\\` package to suppress all warning messages in the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5rcMfFIGiwpt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "711xTBBiiwpt"
      },
      "source": [
        "## Context model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTw3FK_wiwpu"
      },
      "source": [
        "The context model for the reference game is the same as we used before (in Sheet 1.1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "e9KsfKPsiwpu"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## data to fit\n",
        "##################################################\n",
        "\n",
        "object_names     = ['blue_circle', 'green_square', 'blue_square']\n",
        "utterance_names  = ['blue', 'circle', 'green', 'square']\n",
        "semantic_meaning = torch.tensor(\n",
        "    # blue circle, green square, blue square\n",
        "    [[1, 0, 1],  # blue\n",
        "     [1, 0, 0],  # circle\n",
        "     [0, 1, 0],  # green\n",
        "     [0, 1, 1]],  # square,\n",
        "    dtype= torch.float32\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdToZCcOiwpw"
      },
      "source": [
        "## The empirical data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EszZ_Jiwpx"
      },
      "source": [
        "We use empirical data from [Qing & Franke (2016)](https://michael-franke.github.io/heimseite/Papers/QingFranke_2013_Variations_on_Bayes.pdf).\n",
        "There were three tasks: (i) speaker production choice, and (ii) listener interpretation choice, and (iii) salience prior elicitation.\n",
        "All three tasks were *forced-choice tasks*, in which participants had to select a single option from a small list of options.\n",
        "\n",
        "In the speaker production task, participants were presented with the three referents.\n",
        "They were told which object they should refer to.\n",
        "They selected one option from the list of available utterances.\n",
        "\n",
        "In the listener interpretation task, participants were presented with the three referents and an utterance.\n",
        "They selected the object that they thought the speaker meant to refer to with that utterance.\n",
        "\n",
        "In the salience prior elicitation task, participants again saw all three referents.\n",
        "They were told that the speaker wanted to refer to one of these objects with a word in a language they did not know.\n",
        "Again, they were asked to select the object they thought the speaker wanted to refer to.\n",
        "Since this task rids all reasoning about semantic meaning, it is argued to represent a salience baseline of which object is a likely topic of conversation.\n",
        "\n",
        "We use the data from the salience prior condition to feed into the pragmatic listener model.\n",
        "The data from the speaker production and the listener interpretation tasks is our training data, i.e., what we want to explain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KGD6iavWiwpy"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## data to fit\n",
        "##################################################\n",
        "\n",
        "salience_prior = F.normalize(torch.tensor([71,139,30],\n",
        "                                          dtype = torch.float32),\n",
        "                             p = 1, dim = 0)\n",
        "\n",
        "# matrix of number of utterance choices for each state\n",
        "# (rows: objects, columns: utterances)\n",
        "production_data = torch.tensor([[9, 135, 0, 0],\n",
        "                                [0, 0, 119, 25],\n",
        "                                [63, 0, 0, 81]])\n",
        "\n",
        "# matrix of number of object choices for each ambiguous utterance\n",
        "# (rows: utterances, columns: objects)\n",
        "interpretation_data = torch.tensor([[66, 0, 115],   # \"blue\"\n",
        "                                    [0, 117, 62]])  # \"square\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9m3YlOkiwpy"
      },
      "source": [
        "## The RSA model (in PyTorch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzI8BQzniwpz"
      },
      "source": [
        "Here is an implementation of the vanilla RSA model in PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "N2qhoz1hiwpz",
        "outputId": "c8dc03af-5368-4fec-c180-b28bc15b2eb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speaker predictions:\n",
            " tensor([[0.0917, 0.9083, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.2876, 0.7124],\n",
            "        [0.1680, 0.0000, 0.0000, 0.8320]])\n",
            "listener predictions:\n",
            " tensor([[0.5637, 0.0000, 0.4363],\n",
            "        [1.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0000, 0.0000],\n",
            "        [0.0000, 0.7987, 0.2013]])\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## RSA model (forward pass)\n",
        "##################################################\n",
        "\n",
        "def RSA(alpha, cost_adjectives):\n",
        "    costs = torch.tensor([1.0, 0, 1.0, 0]) * cost_adjectives\n",
        "    literal_listener   = F.normalize(semantic_meaning, p = 1, dim = 1)\n",
        "    pragmatic_speaker  = F.normalize(torch.t(literal_listener)**alpha *\n",
        "                                     torch.exp(-alpha * costs), p = 1, dim = 1)\n",
        "    pragmatic_listener = F.normalize(torch.t(pragmatic_speaker) * salience_prior, p = 1, dim = 1)\n",
        "    return({'speaker': pragmatic_speaker, 'listener': pragmatic_listener})\n",
        "\n",
        "print(\"speaker predictions:\\n\", RSA(1, 1.6)['speaker'])\n",
        "print(\"listener predictions:\\n\", RSA(1, 1.6)['listener'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qqcm-Ddiwp0"
      },
      "source": [
        "## Parameters to optimize\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTmwdR9fiwp0"
      },
      "source": [
        "The vanilla RSA model has two free parameters: the optimality parameter $\\alpha$ and the parameter for the cost of utterance, here restricted to a single number for the cost of an adjective (relative to a noun).\n",
        "Since we want to optimize the value of these variables, we require PyTorch to compute gradients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Jq-57PLyiwp1"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## model parameters to fit\n",
        "##################################################\n",
        "\n",
        "alpha           = torch.tensor(1.0, requires_grad=True) # soft-max parameter\n",
        "cost_adjectives = torch.tensor(0.0, requires_grad=True) # differential cost of 'adjectives'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAHF6_0yiwp1"
      },
      "source": [
        "## Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rJqvi6Fiwp1"
      },
      "source": [
        "To optimize the model parameters with stochastic gradient descent, we first instantiate an optimizer object, which we tell about the parameter to optimize.\n",
        "The we iterate the training cycle, each time calling the RSA model (feed-forward pass) with the current parameter values, and then computing the (negative) log-likelihood of the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Mevjn1X2iwp1",
        "outputId": "df58cfd3-25b4-4fa7-ef02-917bae4bfbd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " step                     loss           alpha            cost\n",
            "  250                 21.74205         2.12154         0.17193\n",
            "  500                 16.10578         2.47786         0.15869\n",
            "  750                 15.55774         2.58906         0.15650\n",
            " 1000                 15.50401         2.62389         0.15597\n",
            " 1250                 15.49874         2.63481         0.15582\n",
            " 1500                 15.49818         2.63825         0.15577\n",
            " 1750                 15.49814         2.63933         0.15576\n",
            " 2000                 15.49815         2.63966         0.15575\n",
            " 2250                 15.49814         2.63977         0.15575\n",
            " 2500                 15.49814         2.63979         0.15575\n",
            " 2750                 15.49814         2.63979         0.15575\n",
            " 3000                 15.49814         2.63979         0.15575\n",
            " 3250                 15.49814         2.63979         0.15575\n",
            " 3500                 15.49814         2.63979         0.15575\n",
            " 3750                 15.49814         2.63979         0.15575\n",
            " 4000                 15.49814         2.63979         0.15575\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## optimization\n",
        "##################################################\n",
        "\n",
        "opt = torch.optim.SGD([alpha, cost_adjectives], lr = 0.0001)\n",
        "\n",
        "# output header\n",
        "print('\\n%5s %24s %15s %15s' %\n",
        "      (\"step\", \"loss\", \"alpha\", \"cost\") )\n",
        "\n",
        "for i in range(4000):\n",
        "\n",
        "    RSA_prediction      = RSA(alpha, cost_adjectives)\n",
        "    speaker_pred        = RSA_prediction['speaker']\n",
        "    Multinomial_speaker = torch.distributions.multinomial.Multinomial(144, probs = speaker_pred)\n",
        "    logProbs_speaker    = Multinomial_speaker.log_prob(production_data)\n",
        "\n",
        "    listener_pred          = RSA_prediction['listener']\n",
        "    Multinomial_listener_0 = torch.distributions.multinomial.Multinomial(181,probs = listener_pred[0,])\n",
        "    logProbs_listener_0    = Multinomial_listener_0.log_prob(interpretation_data[0,])\n",
        "    Multinomial_listener_1 = torch.distributions.multinomial.Multinomial(179,probs = listener_pred[3,])\n",
        "    logProbs_listener_1    = Multinomial_listener_1.log_prob(interpretation_data[1,])\n",
        "\n",
        "    loss = -torch.sum(logProbs_speaker) - logProbs_listener_0 - logProbs_listener_1\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    if (i+1) % 250 == 0:\n",
        "        print('%5d %24.5f %15.5f %15.5f' %\n",
        "              (i + 1, loss.item(), alpha.item(),\n",
        "               cost_adjectives.item()) )\n",
        "\n",
        "    opt.step()\n",
        "    opt.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXDyXh8Riwp2"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.3.1: Comparing model variants </span></strong>\n",
        "> 1. We have so far implemented the literal listener as $P_{lit}(s \\mid u) \\propto L_{ij}$. But some RSA models also include the salience prior, which we have so far only used in the pragmatic listener part into the literal listener model. Under this alternative construction the literal listener would be defined as $P_{lit}(s \\mid u) \\propto P_{sal}(s) \\ L_{ij}$. Change the \\`RSA\\` function to implement this alternative definition. (Hint: you only need to add this string somewhere in the code: \\`\\* salience<sub>prior</sub>\\`.) Run the model otherwise as is. Inspect the output of the optimization loop. Use this information to draw conclusions about which of the two model variants is a better predictor of the data.\n",
        "> 2. Go back to the original model. We now want to address whether we actually need the cost parameter. Run the original model (w/ a literal listener w/o salience prior information), but optimize only the $\\alpha$ parameter. The cost parameter should be initialized to 0 and stay this way. Fit the model and use the output information to draw conclusions about which model is better: with or without a flexible cost parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.3.1.1\n",
        "\n",
        "# change RSA function\n",
        "def RSA(alpha, cost_adjectives):\n",
        "    costs = torch.tensor([1.0, 0, 1.0, 0]) * cost_adjectives\n",
        "    literal_listener   = F.normalize(semantic_meaning * salience_prior, p = 1, dim = 1) # add \"*salience_prior\"\n",
        "    pragmatic_speaker  = F.normalize(torch.t(literal_listener)**alpha *\n",
        "                                     torch.exp(-alpha * costs), p = 1, dim = 1)\n",
        "    pragmatic_listener = F.normalize(torch.t(pragmatic_speaker) * salience_prior, p = 1, dim = 1)\n",
        "    return({'speaker': pragmatic_speaker, 'listener': pragmatic_listener})\n",
        "\n",
        "# model parameters to fit\n",
        "alpha           = torch.tensor(1.0, requires_grad=True) # soft-max parameter\n",
        "cost_adjectives = torch.tensor(0.0, requires_grad=True) # differential cost of 'adjectives'\n",
        "\n",
        "# optimization\n",
        "opt = torch.optim.SGD([alpha, cost_adjectives], lr = 0.0001)\n",
        "# output header\n",
        "print('\\n%5s %24s %15s %15s' %\n",
        "      (\"step\", \"loss\", \"alpha\", \"cost\") )\n",
        "\n",
        "for i in range(4000):\n",
        "\n",
        "    RSA_prediction      = RSA(alpha, cost_adjectives)\n",
        "    speaker_pred        = RSA_prediction['speaker']\n",
        "    Multinomial_speaker = torch.distributions.multinomial.Multinomial(144, probs = speaker_pred)\n",
        "    logProbs_speaker    = Multinomial_speaker.log_prob(production_data)\n",
        "\n",
        "    listener_pred          = RSA_prediction['listener']\n",
        "    Multinomial_listener_0 = torch.distributions.multinomial.Multinomial(181,probs = listener_pred[0,])\n",
        "    logProbs_listener_0    = Multinomial_listener_0.log_prob(interpretation_data[0,])\n",
        "    Multinomial_listener_1 = torch.distributions.multinomial.Multinomial(179,probs = listener_pred[3,])\n",
        "    logProbs_listener_1    = Multinomial_listener_1.log_prob(interpretation_data[1,])\n",
        "\n",
        "    loss = -torch.sum(logProbs_speaker) - logProbs_listener_0 - logProbs_listener_1\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    if (i+1) % 250 == 0:\n",
        "        print('%5d %24.5f %15.5f %15.5f' %\n",
        "              (i + 1, loss.item(), alpha.item(),\n",
        "               cost_adjectives.item()) )\n",
        "\n",
        "    opt.step()\n",
        "    opt.zero_grad()"
      ],
      "metadata": {
        "id": "nDvuudZLrnll",
        "outputId": "edfb8ffd-ecca-487b-c0a1-67ea5925f61a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " step                     loss           alpha            cost\n",
            "  250                111.29079         1.58073         0.36963\n",
            "  500                107.47263         1.88243         0.33237\n",
            "  750                106.41679         2.04149         0.31621\n",
            " 1000                106.12294         2.12550         0.30860\n",
            " 1250                106.04050         2.17002         0.30477\n",
            " 1500                106.01729         2.19366         0.30279\n",
            " 1750                106.01073         2.20623         0.30176\n",
            " 2000                106.00888         2.21292         0.30121\n",
            " 2250                106.00835         2.21648         0.30092\n",
            " 2500                106.00819         2.21838         0.30077\n",
            " 2750                106.00813         2.21939         0.30068\n",
            " 3000                106.00813         2.21993         0.30064\n",
            " 3250                106.00813         2.22022         0.30062\n",
            " 3500                106.00813         2.22037         0.30060\n",
            " 3750                106.00813         2.22044         0.30060\n",
            " 4000                106.00810         2.22050         0.30059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer to Exercise 2.3.1.1\n",
        "As can be seen from the results above, after including the salience prior for literal listener, the loss of the model became way larger than before. The loss of the original model was only about 15.5, while right now it is at 106. Which means the predictions of the original model are way closer to train data.\n",
        "\n",
        "Therefore, the original model is way better of a fit than the current one that includes salience prior for literal listener."
      ],
      "metadata": {
        "id": "LSgCP6Mr0kJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.3.1.2\n",
        "\n",
        "# change back to the RSA function\n",
        "def RSA(alpha, cost_adjectives):\n",
        "    costs = torch.tensor([1.0, 0, 1.0, 0]) * cost_adjectives\n",
        "    literal_listener   = F.normalize(semantic_meaning, p = 1, dim = 1) \n",
        "    pragmatic_speaker  = F.normalize(torch.t(literal_listener)**alpha *\n",
        "                                     torch.exp(-alpha * costs), p = 1, dim = 1)\n",
        "    pragmatic_listener = F.normalize(torch.t(pragmatic_speaker) * salience_prior, p = 1, dim = 1)\n",
        "    return({'speaker': pragmatic_speaker, 'listener': pragmatic_listener})\n",
        "\n",
        "# model parameters to fit (cost_adjectives set to 0 and stay this way)\n",
        "alpha           = torch.tensor(1.0, requires_grad=True) # soft-max parameter\n",
        "cost_adjectives = torch.tensor(0.0, requires_grad=True) # differential cost of 'adjectives'\n",
        "\n",
        "# optimization (optimize only the alpha parameter)\n",
        "opt = torch.optim.SGD([alpha], lr = 0.0001)\n",
        "# output header\n",
        "print('\\n%5s %24s %15s %15s' %\n",
        "      (\"step\", \"loss\", \"alpha\", \"cost\") )\n",
        "\n",
        "for i in range(4000):\n",
        "\n",
        "    RSA_prediction      = RSA(alpha, cost_adjectives)\n",
        "    speaker_pred        = RSA_prediction['speaker']\n",
        "    Multinomial_speaker = torch.distributions.multinomial.Multinomial(144, probs = speaker_pred)\n",
        "    logProbs_speaker    = Multinomial_speaker.log_prob(production_data)\n",
        "\n",
        "    listener_pred          = RSA_prediction['listener']\n",
        "    Multinomial_listener_0 = torch.distributions.multinomial.Multinomial(181,probs = listener_pred[0,])\n",
        "    logProbs_listener_0    = Multinomial_listener_0.log_prob(interpretation_data[0,])\n",
        "    Multinomial_listener_1 = torch.distributions.multinomial.Multinomial(179,probs = listener_pred[3,])\n",
        "    logProbs_listener_1    = Multinomial_listener_1.log_prob(interpretation_data[1,])\n",
        "\n",
        "    loss = -torch.sum(logProbs_speaker) - logProbs_listener_0 - logProbs_listener_1\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    if (i+1) % 250 == 0:\n",
        "        print('%5d %24.5f %15.5f %15.5f' %\n",
        "              (i + 1, loss.item(), alpha.item(),\n",
        "               cost_adjectives.item()) )\n",
        "\n",
        "    opt.step()\n",
        "    opt.zero_grad()"
      ],
      "metadata": {
        "id": "qpKkUUDq1YPz",
        "outputId": "d41985f0-a6fe-4f3a-a9ac-49698b5a81c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " step                     loss           alpha            cost\n",
            "  250                 28.00319         2.09809         0.00000\n",
            "  500                 22.72771         2.44209         0.00000\n",
            "  750                 22.25371         2.54512         0.00000\n",
            " 1000                 22.21152         2.57586         0.00000\n",
            " 1250                 22.20776         2.58503         0.00000\n",
            " 1500                 22.20743         2.58776         0.00000\n",
            " 1750                 22.20741         2.58857         0.00000\n",
            " 2000                 22.20741         2.58882         0.00000\n",
            " 2250                 22.20738         2.58889         0.00000\n",
            " 2500                 22.20740         2.58890         0.00000\n",
            " 2750                 22.20740         2.58890         0.00000\n",
            " 3000                 22.20740         2.58890         0.00000\n",
            " 3250                 22.20740         2.58890         0.00000\n",
            " 3500                 22.20740         2.58890         0.00000\n",
            " 3750                 22.20740         2.58890         0.00000\n",
            " 4000                 22.20740         2.58890         0.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer to Exercise 2.3.1.2\n",
        "Based on the results above, after after excluding the cost parameter, the loss of the model became slightly higher, from 15.5 to 22.2, which means the predictions deviate more from the train data now that only the alpha parameter being optimized.\n",
        "\n",
        "So we can conclude that the original model is better -- the model with a flexible cost parameter.\n"
      ],
      "metadata": {
        "id": "JpRht8RE4Gdy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kPmf6vDiwp2"
      },
      "source": [
        "## References\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpysot0kiwp2"
      },
      "source": [
        "Qing, C., & Franke, M. (2015). [Variations on a Bayesian theme: Comparing Bayesian models of referential reasoning](https://michael-franke.github.io/heimseite/Papers/QingFranke_2013_Variations_on_Bayes.pdf). In H. Zeevat, & H. Schmitz (Eds.), Bayesian Natural Language Semantics and Pragmatics (pp. 201–220). Berlin: Springer.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}