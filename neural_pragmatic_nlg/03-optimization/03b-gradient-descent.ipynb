{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2lZToCpdEm"
      },
      "source": [
        "Sheet 3.1: Gradient descent by hand\n",
        "===================================\n",
        "\n",
        "**Author:** Michael Franke\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7HWe0KtpdEy"
      },
      "source": [
        "This short notebook will optimize a parameter with gradient descent without using PyTorch&rsquo;s optimizer.\n",
        "The purpose of this is to demonstrate how vanilla GD works under the hood.\n",
        "We use the previous example of finding the MLE for a Gaussian mean.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student: Jia Sheng (5371477)"
      ],
      "metadata": {
        "id": "020al7lUxQq6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9AsQmqpdE0"
      },
      "source": [
        "## Packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Evl_rD4pdE2"
      },
      "source": [
        "We will need the usual packages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_YreIyHwpdE3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_dtype(torch.float32)"
      ],
      "metadata": {
        "id": "Nhy82TYnyiPB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7D4mBNDpdE8"
      },
      "source": [
        "## Training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaWAc4GkpdE-"
      },
      "source": [
        "The training data are \\`nObs\\` samples from a standard normal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SyUSogwmpdFA"
      },
      "outputs": [],
      "source": [
        "nObs           = 10000\n",
        "trueLocation   = 0 # mean of a normal\n",
        "trueDist       = torch.distributions.Normal(loc=trueLocation, scale=1.0)\n",
        "trainData      = trueDist.sample([nObs])\n",
        "empirical_mean = torch.mean(trainData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bko-b712pdFC"
      },
      "source": [
        "## Training by manual gradient descent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrsu4nZdpdFF"
      },
      "source": [
        "We will actually train two parameters on the same data in parallel.\n",
        "\\`location\\` will be updated by hand; \\`location2\\` will be updated with PyTorch&rsquo;s \\`SGD\\` optimizer.\n",
        "We will use the same learning rate for both.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TNmuuzuspdFH"
      },
      "outputs": [],
      "source": [
        "location       = torch.tensor(1.0, requires_grad=True)\n",
        "location2      = torch.tensor(1.0, requires_grad=True)\n",
        "learningRate   = 0.00001\n",
        "nTrainingSteps = 100\n",
        "opt = torch.optim.SGD([location2], lr = learningRate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05VrchxKpdFJ"
      },
      "source": [
        "The training loop here first updates by hand, then using the built-in\\`SGD\\`.\n",
        "Every 5 rounds we output the current value of \\`location\\` and \\`location2\\`, as well as the difference between them.\n",
        "\n",
        "But, oh no! What&rsquo;s this? There must be a bunch of mistakes in this code! See Exercise below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHE7_dR0pdFL",
        "outputId": "6e67aa3f-09d4-4577-bb86-82637a9f9054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " step        estimate       estimate2      difference\n",
            "\n",
            "    5 0.59388077259064 0.59388077259064 0.00000000000000\n",
            "\n",
            "   10 0.35407140851021 0.35407140851021 0.00000000000000\n",
            "\n",
            "   15 0.21246637403965 0.21246637403965 0.00000000000000\n",
            "\n",
            "   20 0.12885002791882 0.12885002791882 0.00000000000000\n",
            "\n",
            "   25 0.07947541028261 0.07947541028261 0.00000000000000\n",
            "\n",
            "   30 0.05032019317150 0.05032019317150 0.00000000000000\n",
            "\n",
            "   35 0.03310432657599 0.03310432657599 0.00000000000000\n",
            "\n",
            "   40 0.02293853089213 0.02293853089213 0.00000000000000\n",
            "\n",
            "   45 0.01693572849035 0.01693573035300 -0.00000000186265\n",
            "\n",
            "   50 0.01339113619179 0.01339113712311 -0.00000000093132\n",
            "\n",
            "   55 0.01129808928818 0.01129808928818 0.00000000000000\n",
            "\n",
            "   60 0.01006216462702 0.01006216462702 0.00000000000000\n",
            "\n",
            "   65 0.00933236535639 0.00933236535639 0.00000000000000\n",
            "\n",
            "   70 0.00890142377466 0.00890142377466 0.00000000000000\n",
            "\n",
            "   75 0.00864695757627 0.00864695757627 0.00000000000000\n",
            "\n",
            "   80 0.00849669799209 0.00849669799209 0.00000000000000\n",
            "\n",
            "   85 0.00840797461569 0.00840797461569 0.00000000000000\n",
            "\n",
            "   90 0.00835558306426 0.00835558306426 0.00000000000000\n",
            "\n",
            "   95 0.00832464359701 0.00832464359701 0.00000000000000\n",
            "\n",
            "  100 0.00830637570471 0.00830637570471 0.00000000000000\n"
          ]
        }
      ],
      "source": [
        "print('\\n%5s %15s %15s %15s' %\n",
        "      (\"step\", \"estimate\", \"estimate2\", \"difference\") )\n",
        "\n",
        "for i in range(nTrainingSteps):\n",
        "\n",
        "    # manual computation\n",
        "    prediction = torch.distributions.Normal(loc=location, scale=1.0)\n",
        "    loss       = -torch.sum(prediction.log_prob(trainData))\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        # we must embedd this under 'torch.no_grad()' b/c we\n",
        "        # do not want this update state to affect the gradients\n",
        "        location  -= learningRate * location.grad\n",
        "    location.grad = torch.tensor(0.0)\n",
        "\n",
        "    # using PyTorch optimizer\n",
        "    prediction2 = torch.distributions.Normal(loc=location2, scale=1.0)\n",
        "    loss2       = -torch.sum(prediction2.log_prob(trainData))\n",
        "    loss2.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # print output\n",
        "    if (i+1) % 5 == 0:\n",
        "        print('\\n%5s %-2.14f %-2.14f %2.14f' %\n",
        "              (i + 1, location.item(), location2.item(),\n",
        "               location.item() - location2.item()) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q-3KwrUpdFP"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.1: Understand vanilla gradient descent</span></strong>\n",
        ">\n",
        "> Find and correct all mistakes in this code block.\n",
        "> When you are done, the parameters should show no difference at any update step, and they should both converge to the empirical mean.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer to Exercise 3.1.1\n",
        "see the code block above where 2 mistakes in code are corrected."
      ],
      "metadata": {
        "id": "qCfQ7qj8wxpx"
      }
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}