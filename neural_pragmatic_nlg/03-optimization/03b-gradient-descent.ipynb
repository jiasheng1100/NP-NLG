{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2lZToCpdEm"
      },
      "source": [
        "Sheet 3.1: Gradient descent by hand\n",
        "===================================\n",
        "\n",
        "**Author:** Michael Franke\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7HWe0KtpdEy"
      },
      "source": [
        "This short notebook will optimize a parameter with gradient descent without using PyTorch&rsquo;s optimizer.\n",
        "The purpose of this is to demonstrate how vanilla GD works under the hood.\n",
        "We use the previous example of finding the MLE for a Gaussian mean.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student: Jia Sheng (5371477)"
      ],
      "metadata": {
        "id": "020al7lUxQq6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9AsQmqpdE0"
      },
      "source": [
        "## Packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Evl_rD4pdE2"
      },
      "source": [
        "We will need the usual packages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_YreIyHwpdE3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_dtype(torch.float64)"
      ],
      "metadata": {
        "id": "Nhy82TYnyiPB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7D4mBNDpdE8"
      },
      "source": [
        "## Training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaWAc4GkpdE-"
      },
      "source": [
        "The training data are \\`nObs\\` samples from a standard normal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SyUSogwmpdFA"
      },
      "outputs": [],
      "source": [
        "nObs           = 10000\n",
        "trueLocation   = 0 # mean of a normal\n",
        "trueDist       = torch.distributions.Normal(loc=trueLocation, scale=1.0)\n",
        "trainData      = trueDist.sample([nObs])\n",
        "empirical_mean = torch.mean(trainData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bko-b712pdFC"
      },
      "source": [
        "## Training by manual gradient descent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrsu4nZdpdFF"
      },
      "source": [
        "We will actually train two parameters on the same data in parallel.\n",
        "\\`location\\` will be updated by hand; \\`location2\\` will be updated with PyTorch&rsquo;s \\`SGD\\` optimizer.\n",
        "We will use the same learning rate for both.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TNmuuzuspdFH"
      },
      "outputs": [],
      "source": [
        "location       = torch.tensor(1.0, requires_grad=True)\n",
        "location2      = torch.tensor(1.0, requires_grad=True)\n",
        "learningRate   = 0.00001\n",
        "nTrainingSteps = 100\n",
        "opt = torch.optim.SGD([location2], lr = learningRate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05VrchxKpdFJ"
      },
      "source": [
        "The training loop here first updates by hand, then using the built-in\\`SGD\\`.\n",
        "Every 5 rounds we output the current value of \\`location\\` and \\`location2\\`, as well as the difference between them.\n",
        "\n",
        "But, oh no! What&rsquo;s this? There must be a bunch of mistakes in this code! See Exercise below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHE7_dR0pdFL",
        "outputId": "bd160703-82c4-4992-9a9a-239d2aeae9b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " step        estimate       estimate2      difference\n",
            "\n",
            "    5 0.58854341900797 0.58854341900797 0.00000000000000\n",
            "\n",
            "   10 0.34558242249798 0.34558242249798 -0.00000000000000\n",
            "\n",
            "   15 0.20211638366880 0.20211638366880 0.00000000000000\n",
            "\n",
            "   20 0.11740112240056 0.11740112240056 0.00000000000000\n",
            "\n",
            "   25 0.06737760777428 0.06737760777428 0.00000000000000\n",
            "\n",
            "   30 0.03783922262260 0.03783922262260 0.00000000000000\n",
            "\n",
            "   35 0.02039710157439 0.02039710157439 0.00000000000000\n",
            "\n",
            "   40 0.01009770351663 0.01009770351663 0.00000000000000\n",
            "\n",
            "   45 0.00401601195750 0.00401601195750 0.00000000000000\n",
            "\n",
            "   50 0.00042483390876 0.00042483390876 0.00000000000000\n",
            "\n",
            "   55 -0.00169572081725 -0.00169572081725 0.00000000000000\n",
            "\n",
            "   60 -0.00294788717741 -0.00294788717741 0.00000000000000\n",
            "\n",
            "   65 -0.00368727889142 -0.00368727889142 -0.00000000000000\n",
            "\n",
            "   70 -0.00412388230463 -0.00412388230463 -0.00000000000000\n",
            "\n",
            "   75 -0.00438169225409 -0.00438169225409 -0.00000000000000\n",
            "\n",
            "   80 -0.00453392645115 -0.00453392645115 -0.00000000000000\n",
            "\n",
            "   85 -0.00462381922217 -0.00462381922217 0.00000000000000\n",
            "\n",
            "   90 -0.00467690000453 -0.00467690000453 0.00000000000000\n",
            "\n",
            "   95 -0.00470824367571 -0.00470824367571 0.00000000000000\n",
            "\n",
            "  100 -0.00472675180010 -0.00472675180010 0.00000000000000\n"
          ]
        }
      ],
      "source": [
        "print('\\n%5s %15s %15s %15s' %\n",
        "      (\"step\", \"estimate\", \"estimate2\", \"difference\") )\n",
        "\n",
        "for i in range(nTrainingSteps):\n",
        "\n",
        "    # manual computation\n",
        "    prediction = torch.distributions.Normal(loc=location, scale=1.0)\n",
        "    loss       = -torch.sum(prediction.log_prob(trainData))\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        # we must embedd this under 'torch.no_grad()' b/c we\n",
        "        # do not want this update state to affect the gradients\n",
        "        location  -= learningRate * location.grad\n",
        "    location.grad = torch.tensor(0.0)\n",
        "\n",
        "    # using PyTorch optimizer\n",
        "    prediction2 = torch.distributions.Normal(loc=location2, scale=1.0)\n",
        "    loss2       = -torch.sum(prediction2.log_prob(trainData))\n",
        "    loss2.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # print output\n",
        "    if (i+1) % 5 == 0:\n",
        "        print('\\n%5s %-2.14f %-2.14f %2.14f' %\n",
        "              (i + 1, location.item(), location2.item(),\n",
        "               location.item() - location2.item()) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q-3KwrUpdFP"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.1: Understand vanilla gradient descent</span></strong>\n",
        ">\n",
        "> Find and correct all mistakes in this code block.\n",
        "> When you are done, the parameters should show no difference at any update step, and they should both converge to the empirical mean.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer to Exercise 3.1.1\n",
        "see the code block above where 2 mistakes in code are corrected."
      ],
      "metadata": {
        "id": "qCfQ7qj8wxpx"
      }
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}