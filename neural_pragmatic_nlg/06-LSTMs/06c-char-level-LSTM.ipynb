{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIv9tPdE56y8"
      },
      "source": [
        "Sheet 6.2: Character-level sequence modeling w/ LSTMs\n",
        "=====================================================\n",
        "\n",
        "**Author:** Michael Franke\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZEPt7PK56y-"
      },
      "source": [
        "This tutorial builds on the earlier tutorial (5.1) which implemented a character-level RNN.\n",
        "Previously we implemented the RNN model without making use of PyTorch&rsquo;s built-in functions.\n",
        "In this tutorial, we will implement an LSTM using these convenient functions.\n",
        "Applying the new LSTM model to the exact same data (surname predictions for different countries), we can compare the efficiency and power of the two architectures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student: Jia Sheng (5371477)"
      ],
      "metadata": {
        "id": "KQpNUN5j5-qp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igr9BXHd56zA"
      },
      "source": [
        "## Packages & global parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6S6O5rv56zB"
      },
      "source": [
        "Imports as before in (5.1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A1PiI3lN56zC"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## import packages\n",
        "##################################################\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import pandas\n",
        "import string\n",
        "import torch\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC5rnNFz56zD"
      },
      "source": [
        "## Load & pre-process data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI6ggRHN56zE"
      },
      "source": [
        "Loading and pre-processing the data is also as before in sheet 5.1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VRTtn6wQ56zF",
        "outputId": "a843711d-64fe-4c88-b93b-e7f5b64e32c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Czech 519 467 52\n",
            "German 724 652 72\n",
            "Arabic 2000 1800 200\n",
            "Japanese 991 892 99\n",
            "Chinese 268 241 27\n",
            "Vietnamese 73 66 7\n",
            "Russian 9408 8467 941\n",
            "French 277 249 28\n",
            "Irish 232 209 23\n",
            "English 3668 3301 367\n",
            "Spanish 298 268 30\n",
            "Greek 203 183 20\n",
            "Italian 709 638 71\n",
            "Portuguese 74 67 7\n",
            "Scottish 100 90 10\n",
            "Dutch 297 267 30\n",
            "Korean 94 85 9\n",
            "Polish 139 125 14\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## read and inspect the data\n",
        "##################################################\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/michael-franke/npNLG/main/neural_pragmatic_nlg/05-RNNs/names-data.json\") as url:\n",
        "     namesData = json.load(url)\n",
        "\n",
        "# with open('names-data.json') as dataFile:\n",
        "#     namesData = json.load(dataFile)\n",
        "\n",
        "categories = list(namesData.keys())\n",
        "n_categories   = len(categories)\n",
        "\n",
        "# we use all ASCII letters as the vocabulary (plus tokens [EOS], [SOS])\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters   = len(all_letters) + 2 # all letter plus [EOS] and [SOS] token\n",
        "SOSIndex    = n_letters - 1\n",
        "EOSIndex    = n_letters - 2\n",
        "\n",
        "##################################################\n",
        "## make a train/test split\n",
        "##################################################\n",
        "\n",
        "train_data = dict()\n",
        "test_data  = dict()\n",
        "split_percentage = 10\n",
        "for k in list(namesData.keys()):\n",
        "    total_size    = len(namesData[k])\n",
        "    test_size     = round(total_size/split_percentage)\n",
        "    train_size    = total_size - test_size\n",
        "    print(k, total_size, train_size, test_size)\n",
        "    indices       = [i for i in range(total_size)]\n",
        "    random.shuffle(indices)\n",
        "    train_indices = indices[0:train_size]\n",
        "    test_indices  = indices[(train_size+1):(-1)]\n",
        "    train_data[k] = [namesData[k][i] for i in train_indices]\n",
        "    test_data[k]  = [namesData[k][i] for i in test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDTNs3zs56zG"
      },
      "source": [
        "## Define LSTM module\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUow9Vt356zI"
      },
      "source": [
        "The definition of the LSTM model follows the steps explained in the previous worksheet (6.1) closely.\n",
        "NB: we include a dropout rate (which here acts in between layers of the stacked LSTM).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BXfhNM6z56zJ"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## define LSTM\n",
        "##################################################\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, cat_embedding_size, n_cat,\n",
        "                 char_embedding_size, n_char,\n",
        "                 hidden_size, output_size, num_layers = 2, dropout = 0.1):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # category embedding\n",
        "        self.cat_embedding = nn.Embedding(n_cat, cat_embedding_size)\n",
        "        # character embedding\n",
        "        self.char_embedding = nn.Embedding(n_char, char_embedding_size)\n",
        "        # the actual LSTM\n",
        "        self.lstm = nn.LSTM(input_size  = cat_embedding_size+char_embedding_size,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers  = num_layers,\n",
        "                            batch_first = True,\n",
        "                            dropout = dropout\n",
        "                            )\n",
        "        # linear map onto weights for words\n",
        "        self.linear_map = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, category, name, hidden):\n",
        "        cat_emb  = self.cat_embedding(category)\n",
        "        char_emb = self.char_embedding(name)\n",
        "        output, (hidden, cell) = self.lstm(torch.concat([cat_emb, char_emb], dim = 1))\n",
        "        predictions = self.linear_map(output)\n",
        "        return torch.nn.functional.log_softmax(predictions, dim = 1), hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RknW2_PU56zK"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.2.1: </span></strong>\n",
        ">\n",
        "> 1. How is the category information supplied to next network? I.e.,, what is the input format and how is this information made accessible for computation at every word?\n",
        ">\n",
        "> 2. What exactly is the return value of a single forward pass?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answers: \n",
        "**Exercise 6.2.1.1**\n",
        "\n",
        "- The category information was a embedding matrix consisting of vectors of embeddings, the size of each vector is specified as cat_embedding_size.\n",
        "- The size of this embedding matrix is the same as the number of characters in the name, i.e. the embedding vector is repeated in this matrix as many times as the length of the random string name in this random training pair. \n",
        "- Then, the category embedding matrix is concatenated to the character embedding matrix and passed to the LSTM function, so that each character has access to the category information.\n",
        "\n",
        "\n",
        "**Exercise 6.2.1.2**\n",
        "\n",
        "- The output of the LSTM function consists of the last layer of each character embeddings, the hidden state of all layers of the last character, and the cell state of all layers of the last character.\n",
        "- Then, within the forward function, the embeddings of the last layer of each character embedding is mapped to a vector of weights (predictions) of the same length as the size of all letters.\n",
        "- These predictions are then normalized by a softmax function and turned into the next-character probabilities, that is, each character is represented as a probability value of it being the next character. Along with the hidden state of all layers of the last character it composed the return value of this single forward pass.\n"
      ],
      "metadata": {
        "id": "HX7YLPbmEUjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the embedding output\n",
        "cat_embedding = nn.Embedding(18, 32)\n",
        "cat_embedding(torch.tensor([4, 4, 4, 4, 4]))\n"
      ],
      "metadata": {
        "id": "SAzr9dlY8gKf",
        "outputId": "4a339714-4cc7-4607-97d3-6e8052086aea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.8047,  0.0885,  0.4692,  0.5301,  1.1307, -0.6709,  1.4770, -1.3666,\n",
              "          0.4251, -0.1662,  1.0761,  1.3686, -0.3516, -0.9902,  0.9859, -1.0888,\n",
              "          0.2001, -0.6627,  1.8545,  0.4255, -0.2603,  1.7848,  0.5559, -1.2131,\n",
              "         -0.5495, -1.1968, -2.4137,  0.0458,  0.2493,  0.3542,  0.8570,  0.2510],\n",
              "        [ 1.8047,  0.0885,  0.4692,  0.5301,  1.1307, -0.6709,  1.4770, -1.3666,\n",
              "          0.4251, -0.1662,  1.0761,  1.3686, -0.3516, -0.9902,  0.9859, -1.0888,\n",
              "          0.2001, -0.6627,  1.8545,  0.4255, -0.2603,  1.7848,  0.5559, -1.2131,\n",
              "         -0.5495, -1.1968, -2.4137,  0.0458,  0.2493,  0.3542,  0.8570,  0.2510],\n",
              "        [ 1.8047,  0.0885,  0.4692,  0.5301,  1.1307, -0.6709,  1.4770, -1.3666,\n",
              "          0.4251, -0.1662,  1.0761,  1.3686, -0.3516, -0.9902,  0.9859, -1.0888,\n",
              "          0.2001, -0.6627,  1.8545,  0.4255, -0.2603,  1.7848,  0.5559, -1.2131,\n",
              "         -0.5495, -1.1968, -2.4137,  0.0458,  0.2493,  0.3542,  0.8570,  0.2510],\n",
              "        [ 1.8047,  0.0885,  0.4692,  0.5301,  1.1307, -0.6709,  1.4770, -1.3666,\n",
              "          0.4251, -0.1662,  1.0761,  1.3686, -0.3516, -0.9902,  0.9859, -1.0888,\n",
              "          0.2001, -0.6627,  1.8545,  0.4255, -0.2603,  1.7848,  0.5559, -1.2131,\n",
              "         -0.5495, -1.1968, -2.4137,  0.0458,  0.2493,  0.3542,  0.8570,  0.2510],\n",
              "        [ 1.8047,  0.0885,  0.4692,  0.5301,  1.1307, -0.6709,  1.4770, -1.3666,\n",
              "          0.4251, -0.1662,  1.0761,  1.3686, -0.3516, -0.9902,  0.9859, -1.0888,\n",
              "          0.2001, -0.6627,  1.8545,  0.4255, -0.2603,  1.7848,  0.5559, -1.2131,\n",
              "         -0.5495, -1.1968, -2.4137,  0.0458,  0.2493,  0.3542,  0.8570,  0.2510]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX48MQAU56zK"
      },
      "source": [
        "## Helper functions for training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KpJseM_56zK"
      },
      "source": [
        "Again, the following training functions are similar to what we used in sheet 5.1, but changed to handle the different representational format of the input.\n",
        "(Previous work sheet used a one-hot vector representation where we here use an index (integer) representation for each word.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oddwfj-R56zL"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## helper functions for training\n",
        "##################################################\n",
        "\n",
        "# Random item from a list\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Get a random category and random name from that category\n",
        "def randomTrainingPair():\n",
        "    category = randomChoice(categories)\n",
        "    name = randomChoice(train_data[category])\n",
        "    return category, name\n",
        "\n",
        "# get index representation of name (in the proper format)\n",
        "def getNameIndices(name):\n",
        "    indices = [SOSIndex] + [all_letters.index(c) for c in list(name)] + [EOSIndex]\n",
        "    return indices\n",
        "\n",
        "# get index representation of category (in the proper format)\n",
        "# NB: must have same length as corresponding name representation b/c\n",
        "#     each character in the sequence is concatenated with the category information\n",
        "def getCatIndices(category, name_length):\n",
        "    return torch.full((1,name_length), categories.index(category)).reshape(-1)\n",
        "\n",
        "# get random training pair in desired input format (vectors of indices)\n",
        "def randomTrainingExample():\n",
        "    category, name = randomTrainingPair()\n",
        "    name_length = len(name) + 2\n",
        "    return getCatIndices(category, name_length), torch.tensor(getNameIndices(name))\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getNameIndices(\"Sheng\")"
      ],
      "metadata": {
        "id": "4Vv48Bs9-1Dp",
        "outputId": "d9344f8d-90bb-4a94-8419-852a2b0f00dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[59, 44, 7, 4, 13, 6, 58]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getCatIndices(\"English\", 7)"
      ],
      "metadata": {
        "id": "ETUyBzzp-99e",
        "outputId": "5ebe78dc-fa32-4d29-ceba-2e2500d3e660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 9, 9, 9, 9, 9, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomTrainingExample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1N0ACnW77DV",
        "outputId": "f438353f-ad2c-4e04-b0f9-460e334d4a9a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([16, 16, 16, 16, 16]), tensor([59, 50, 14, 14, 58]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_letters))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTTIrH5acw0F",
        "outputId": "a082dd30-2e58-48aa-e6e4-cadf05788c88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLGHW1ja56zL"
      },
      "source": [
        "## Single training step\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eergbByI56zM"
      },
      "source": [
        "A single training loop for a single pair of category and name considers the output predictions of the LSTM.\n",
        "The way we defined the LSTM above makes it so that the first component that is returned feeds directly in to the loss function (negative log likeihood).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nD8qT9Ja56zM"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## single training pass\n",
        "##################################################\n",
        "\n",
        "def train(cat, name):\n",
        "    # get a fresh hidden layer\n",
        "    hidden = lstm.initHidden()\n",
        "    # zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # run sequence\n",
        "    predictions, hidden = lstm(cat, name, hidden)\n",
        "    # compute loss (NLLH)\n",
        "    loss = criterion(predictions[:-1], name[1:len(name)])\n",
        "    # perform backward pass\n",
        "    loss.backward()\n",
        "    # perform optimization\n",
        "    optimizer.step()\n",
        "    # return prediction and loss\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmCuxF8756zN"
      },
      "source": [
        "## Model instantiation & training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJfvRZzB56zN"
      },
      "source": [
        "The LSTM we instantiate here is rather smallish.\n",
        "It has only one layer, a hidden and cell state of size 64 and uses an embedding size of 32 for both categories and names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "liZVn0Q356zN",
        "outputId": "30e14365-ed3b-4c33-e4dc-e8b76a8cbc27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0m 15s (5000 10%) 16.9360\n",
            "0m 31s (10000 20%) 16.0445\n",
            "0m 46s (15000 30%) 15.5689\n",
            "1m 3s (20000 40%) 15.2217\n",
            "1m 20s (25000 50%) 15.0159\n",
            "1m 37s (30000 60%) 14.8340\n",
            "1m 52s (35000 70%) 14.6694\n",
            "2m 8s (40000 80%) 14.5188\n",
            "2m 24s (45000 90%) 14.4107\n",
            "2m 40s (50000 100%) 14.3063\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## actual training loop\n",
        "## (should take about 1-2 minutes)\n",
        "##################################################\n",
        "\n",
        "# instantiate model\n",
        "lstm = LSTM(cat_embedding_size  = 32,\n",
        "            n_cat               = n_categories,\n",
        "            char_embedding_size = 32,\n",
        "            n_char              = n_letters,\n",
        "            hidden_size         = 64,\n",
        "            output_size         = n_letters,\n",
        "            dropout             = 0,\n",
        "            num_layers          = 1\n",
        "            )\n",
        "# training objective\n",
        "criterion = nn.NLLLoss(reduction='sum')\n",
        "# learning rate\n",
        "learning_rate = 0.005\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "# training parameters\n",
        "n_iters = 50000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0 # will be reset every 'plot_every' iterations\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        rolling_mean = np.mean(all_losses[iter - print_every*(iter//print_every):])\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start),\n",
        "                                     iter,\n",
        "                                     iter / n_iters * 100,\n",
        "                                     rolling_mean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_f5oaRw56zP"
      },
      "source": [
        "## Plotting training performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S339op6I56zQ",
        "outputId": "c8e5967e-611c-4815-b651-e400c4bfc0cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "##################################################\n",
        "## monitoring loss function during training\n",
        "##################################################\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zig6Ibeh56zQ"
      },
      "source": [
        "## Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5z3q4zP56zR"
      },
      "source": [
        "Extraction of surprisal of a single item and computation of average surprisal for test and training set is largely parallel to the case of sheet 5.1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_4pCtp5i56zR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126c2f8a-e1a4-4b0e-fb6c-c40315047cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "mean surprisal (test): 16.04405969481709\n",
            "\n",
            "mean surprisal (train): 13.403144764069278\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## evaluation\n",
        "##################################################\n",
        "\n",
        "def get_surprisal_item(category, name):\n",
        "    name      = torch.tensor(getNameIndices(name))\n",
        "    cat       = getCatIndices(category,len(name))\n",
        "    hidden    = lstm.initHidden()\n",
        "    prediction, hidden = lstm(cat, name, hidden)\n",
        "    nll       = criterion(prediction[:-1], name[1:len(name)])\n",
        "    return(nll.item())\n",
        "\n",
        "def get_surprisal_dataset(data):\n",
        "    surprisl_dict = dict()\n",
        "    surp_avg_dict = dict()\n",
        "    perplxty_dict = dict()\n",
        "    for category in list(data.keys()):\n",
        "        surprisl = 0\n",
        "        surp_avg = 0\n",
        "        perplxty = 0\n",
        "        # training\n",
        "        for name in data[category]:\n",
        "            item_surpr = get_surprisal_item(category, name)\n",
        "            surprisl  += item_surpr\n",
        "            surp_avg  += item_surpr / len(name)\n",
        "            perplxty  += item_surpr ** (-1 / len(name))\n",
        "        n_items = len(data[category])\n",
        "\n",
        "        surprisl_dict[category] = (surprisl /n_items)\n",
        "        surp_avg_dict[category] = (surp_avg / n_items)\n",
        "        perplxty_dict[category] = (perplxty / n_items)\n",
        "\n",
        "    return(surprisl_dict, surp_avg_dict, perplxty_dict)\n",
        "\n",
        "def makeDF(surp_dict):\n",
        "    p = pandas.DataFrame.from_dict(surp_dict)\n",
        "    p = p.transpose()\n",
        "    p.columns = [\"surprisal\", \"surp_scaled\", \"perplexity\"]\n",
        "    return(p)\n",
        "\n",
        "surprisal_test  = makeDF(get_surprisal_dataset(test_data))\n",
        "surprisal_train = makeDF(get_surprisal_dataset(train_data))\n",
        "\n",
        "print(\"\\nmean surprisal (test):\", np.mean(surprisal_test[\"surprisal\"]))\n",
        "print(\"\\nmean surprisal (train):\", np.mean(surprisal_train[\"surprisal\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFN1Vmtj56zR"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.2.2: Interpret the evaluation metrics </span></strong>\n",
        ">\n",
        "> 1. What do you conclude from these two numbers? Is there a chance that the model overfitted the training data?\n",
        ">\n",
        "> 2. What do you conclude about the performance of the RNN (from sheet 5.1) and the current LSTM implementation? Which model is better?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answers:\n",
        "\n",
        "**Exercise 6.2.2.1**\n",
        "\n",
        "- These two numbers represent the average surprisal (cross entropy) of the predictions made by trained model for different input data, when the true distribution is the data from the given dataset. The lower this value, the closer the model predictions are to the true distribution. The average surprisal of train data is lower than that of the test data, suggesting that this model performs better at predicting train data than test data.\n",
        "- There is a chance that the model overfitted the training data, because the average surprisal of train data is muss smaller than the test data, meaning the model performs worse on unseen data than seen data, which indicates overfitting.\n",
        "\n",
        "**Exercise 6.2.2.2**\n",
        "\n",
        "- If we compare the average surprisal of train data, the current LSTM has better performance. The average surprisal of train data by RNN model was 16.6, while by the current model is 13.2, which means the LSTM model's predictions on the train data are much closer to the true distribution.\n",
        "- However, if we compare the difference of the average surprisal between train and test data, the gap is much smaller in the case of the RNN model, which suggests that there is less overfitting with that model. So the RNN model might be more generalizable than the LSTM model."
      ],
      "metadata": {
        "id": "qMa9oaGnBONN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYAHCgaK56zR"
      },
      "source": [
        "## Exploring model predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP7b3gqE56zR"
      },
      "source": [
        "Now the fun part starts!\n",
        "Let&rsquo;s see how the generations of the LSTM look like.\n",
        "Notice that there is a flag for the kind of decoding strategy to be used.\n",
        "Currently, there are two decoding strategies (but see exercise below).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "2obwJj4f56zS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4591680b-43be-4c49-f5c3-43e939e0844c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ster\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## prediction function\n",
        "##################################################\n",
        "\n",
        "max_length = 20\n",
        "\n",
        "# make a prediction based on given sequence\n",
        "def predict(category, initial_sequence, decode_strat = \"greedy\"):\n",
        "\n",
        "    if len(initial_sequence) >= max_length:\n",
        "        return(initial_sequence)\n",
        "\n",
        "    name      = torch.tensor(getNameIndices(initial_sequence))[:-1]\n",
        "    cat       = getCatIndices(category,len(name))\n",
        "    hidden    = lstm.initHidden()\n",
        "\n",
        "    generation = initial_sequence\n",
        "\n",
        "    output, hidden = lstm(cat, name, hidden)\n",
        "    next_word_pred = output[-1]\n",
        "    \n",
        "\n",
        "    if decode_strat == \"pure\":\n",
        "        sample_index = torch.multinomial(input = torch.exp(next_word_pred),\n",
        "                                         num_samples = 1)\n",
        "        pass\n",
        "    else:\n",
        "        topv, topi = next_word_pred.topk(1)\n",
        "        sample_index = topi[0].item()\n",
        "\n",
        "    if sample_index == EOSIndex:\n",
        "        return(generation)\n",
        "    else:\n",
        "        generation += all_letters[sample_index]\n",
        "\n",
        "    return(predict(category, generation))\n",
        "\n",
        "print(predict(\"German\", \"\", decode_strat = \"greedy\"))\n",
        "#print(predict(\"German\", \"\", decode_strat = \"pure\"))\n",
        "#print(predict(\"German\", \"\", decode_strat = \"pure\"))\n",
        "#print(predict(\"German\", \"\", decode_strat = \"pure\"))\n",
        "\n",
        "#print(predict(\"Japanese\", \"\", decode_strat = \"greedy\"))\n",
        "#print(predict(\"Japanese\", \"\", decode_strat = \"pure\"))\n",
        "#print(predict(\"Japanese\", \"\", decode_strat = \"pure\"))\n",
        "#print(predict(\"Japanese\", \"\", decode_strat = \"pure\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(\"Chinese\", \"\", decode_strat = \"greedy\"))\n",
        "print(predict(\"Chinese\", \"\", decode_strat = \"pure\"))\n",
        "print(predict(\"Chinese\", \"\", decode_strat = \"pure\"))\n",
        "print(predict(\"Chinese\", \"\", decode_strat = \"pure\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7WxGVG8GHfD",
        "outputId": "e680311b-8ee0-4817-bf5e-b94e9812db33"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sang\n",
            "Yong\n",
            "Dou\n",
            "Xing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItbeOvK056zS"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.2.3: Predictions under different decoding schemes </span></strong>\n",
        ">\n",
        "> 0. [Just for yourself] Play around with the prediction function. Are you content with the quality of the predictions? Is the model performing better than the previous RNN in your perception?\n",
        ">\n",
        "> 1. Extend the function &rsquo;predict&rsquo; by implementing three additional decoding schemes: top-k, softmax and top-p. Write the function in such a way that the decoding strategy can be chosen by the user with a mnemonic string (like already don for the &ldquo;pure&rdquo; decoding strategy).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answers:\n",
        "\n",
        "**Exercise 6.2.3.0 [just for myself]**\n",
        "\n",
        "- The predictions of this model seem to perform better on Chinese names than the previous RNN model. Unlike last time, most of the names that it outputs are realistic names. \n",
        "- One drawback is, no matter which category one chooses, the greedy decoded prediction always starts with \"S\". I think in some categories it might be the truth that the name is most possible to start with \"S\", but probably not in all categories.\n",
        "\n",
        "**Exercise 6.2.3.1 (see below)**\n"
      ],
      "metadata": {
        "id": "CDwbrtx8Ggg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extend predict function by implementing top-k, softmax and top-p decoding schemes\n",
        "def predict(category, initial_sequence, decode_strat = \"greedy\", k=10, p=0.5):\n",
        "    if len(initial_sequence) >= max_length:\n",
        "        return(initial_sequence)\n",
        "\n",
        "    name      = torch.tensor(getNameIndices(initial_sequence))[:-1]\n",
        "    cat       = getCatIndices(category,len(name))\n",
        "    hidden    = lstm.initHidden()\n",
        "\n",
        "    generation = initial_sequence\n",
        "\n",
        "    output, hidden = lstm(cat, name, hidden)\n",
        "    next_word_pred = output[-1]\n",
        "\n",
        "    softmax = nn.Softmax()\n",
        "\n",
        "    # pure \n",
        "    if decode_strat == \"pure\":\n",
        "        sample_index = torch.multinomial(input = torch.exp(next_word_pred),\n",
        "                                         num_samples = 1)\n",
        "    # implement softmax decoding schema\n",
        "    elif decode_strat == \"softmax\":\n",
        "        # sample based on the softmaxed probabilities\n",
        "        sample_index = torch.multinomial(input = softmax(next_word_pred), num_samples = 1)\n",
        "\n",
        "    # implement top-k decoding schema\n",
        "    elif decode_strat == \"topk\":\n",
        "        # get the top-k values(probabilities) and their indices\n",
        "        topv, topi = next_word_pred.topk(k)\n",
        "        # use random choice to choose one index based on their probabilities among the top-k values\n",
        "        sample_index = np.random.choice(topi.detach().cpu().numpy(), 1, p=softmax(topv).detach().cpu().numpy())[0]\n",
        "\n",
        "    # implement top-p decoding schema\n",
        "    elif decode_strat == \"topp\":\n",
        "        # sort the probabilities by descending order\n",
        "        sortedv, sortedi = torch.sort(softmax(next_word_pred), descending=True)\n",
        "        # calculate cumulative probabilities\n",
        "        cumulative_probs = torch.cumsum(sortedv, dim=0)\n",
        "        # keep the values and indices whose cumulative probabilities are less than the specified p value\n",
        "        to_keep = cumulative_probs <= p\n",
        "        # get the kept values and their indices\n",
        "        keptedv = sortedv[to_keep]\n",
        "        keptedi = sortedi[to_keep]\n",
        "        # use random choice to choose one index based on their probabilities among the top-p values\n",
        "        sample_index = np.random.choice(keptedi.detach().cpu().numpy(), 1, p=softmax(keptedv).detach().cpu().numpy())[0]\n",
        "\n",
        "    # greedy\n",
        "    else:\n",
        "        topv, topi = next_word_pred.topk(1)\n",
        "        sample_index = topi[0].item()\n",
        "\n",
        "    if sample_index == EOSIndex:\n",
        "        return(generation)\n",
        "    else:\n",
        "        generation += all_letters[sample_index]\n",
        "\n",
        "    return(predict(category, generation))\n",
        "\n",
        "print(predict(\"German\", \"\", decode_strat = \"greedy\"))\n",
        "print(predict(\"German\", \"\", decode_strat = \"pure\"))\n",
        "print(predict(\"German\", \"\", decode_strat = \"softmax\"))\n",
        "print(predict(\"German\", \"\", decode_strat = \"topk\"))\n",
        "print(predict(\"German\", \"\", decode_strat = \"topp\"))\n",
        "\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"greedy\"))\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"pure\"))\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"softmax\"))\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"topk\"))\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"topp\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgBkh3jLHXcN",
        "outputId": "f11bbfa3-dd83-439d-81c8-4aa88655a06c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ster\n",
            "Toming\n",
            "Weiner\n",
            "Groser\n",
            "Ster\n",
            "Takama\n",
            "Nakama\n",
            "Kamama\n",
            "Takama\n",
            "Mashi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX_WRgDj56zT"
      },
      "source": [
        "## [Excursion] Class predictions from the generation model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqTU2Yy_56zT"
      },
      "source": [
        "In the previous sheet 5.1 we looked at a way of using the string generation probabilities for categorization.\n",
        "Here is a function that does that, too, but now for the LSTM model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFQrsCn956zT"
      },
      "outputs": [],
      "source": [
        "def infer_category(name):\n",
        "    probs = torch.tensor([torch.exp(-torch.tensor(get_surprisal_item(c, name))) for c in categories])\n",
        "    probs = probs/torch.sum(probs)\n",
        "    vals, cats = probs.topk(3)\n",
        "    print(\"Top 3 guesses for \", name, \":\\n\")\n",
        "    for i in range(len(cats)):\n",
        "        print(\"%12s: %.5f\" %\n",
        "              (categories[cats[i]], vals[i].detach().numpy() ))\n",
        "\n",
        "infer_category(\"Smith\")\n",
        "infer_category(\"Miller\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiaRm5L456zU"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.2.4: Reflect on derived category predictions </span></strong>\n",
        ">\n",
        "> 1. [This is a bonus exercise; optional!] Check out the model&rsquo;s predictions for &ldquo;Smith&rdquo; and &ldquo;Miller&rdquo;. Is this what you would expect of a categorization function? Why? Why not? Can you explain why the this &ldquo;derived categorization model&rdquo; makes these predictions?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}