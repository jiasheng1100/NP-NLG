{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIv9tPdE56y8"
      },
      "source": [
        "Sheet 6.2: Character-level sequence modeling w/ LSTMs\n",
        "=====================================================\n",
        "\n",
        "**Author:** Michael Franke\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZEPt7PK56y-"
      },
      "source": [
        "This tutorial builds on the earlier tutorial (5.1) which implemented a character-level RNN.\n",
        "Previously we implemented the RNN model without making use of PyTorch&rsquo;s built-in functions.\n",
        "In this tutorial, we will implement an LSTM using these convenient functions.\n",
        "Applying the new LSTM model to the exact same data (surname predictions for different countries), we can compare the efficiency and power of the two architectures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student: Jia Sheng (5371477)"
      ],
      "metadata": {
        "id": "KQpNUN5j5-qp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igr9BXHd56zA"
      },
      "source": [
        "## Packages & global parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6S6O5rv56zB"
      },
      "source": [
        "Imports as before in (5.1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A1PiI3lN56zC"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## import packages\n",
        "##################################################\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import pandas\n",
        "import string\n",
        "import torch\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC5rnNFz56zD"
      },
      "source": [
        "## Load & pre-process data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI6ggRHN56zE"
      },
      "source": [
        "Loading and pre-processing the data is also as before in sheet 5.1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VRTtn6wQ56zF",
        "outputId": "ce518938-6bee-4a78-cbc0-c913771af419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Czech 519 467 52\n",
            "German 724 652 72\n",
            "Arabic 2000 1800 200\n",
            "Japanese 991 892 99\n",
            "Chinese 268 241 27\n",
            "Vietnamese 73 66 7\n",
            "Russian 9408 8467 941\n",
            "French 277 249 28\n",
            "Irish 232 209 23\n",
            "English 3668 3301 367\n",
            "Spanish 298 268 30\n",
            "Greek 203 183 20\n",
            "Italian 709 638 71\n",
            "Portuguese 74 67 7\n",
            "Scottish 100 90 10\n",
            "Dutch 297 267 30\n",
            "Korean 94 85 9\n",
            "Polish 139 125 14\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## read and inspect the data\n",
        "##################################################\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/michael-franke/npNLG/main/neural_pragmatic_nlg/05-RNNs/names-data.json\") as url:\n",
        "     namesData = json.load(url)\n",
        "\n",
        "# with open('names-data.json') as dataFile:\n",
        "#     namesData = json.load(dataFile)\n",
        "\n",
        "categories = list(namesData.keys())\n",
        "n_categories   = len(categories)\n",
        "\n",
        "# we use all ASCII letters as the vocabulary (plus tokens [EOS], [SOS])\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters   = len(all_letters) + 2 # all letter plus [EOS] and [SOS] token\n",
        "SOSIndex    = n_letters - 1\n",
        "EOSIndex    = n_letters - 2\n",
        "\n",
        "##################################################\n",
        "## make a train/test split\n",
        "##################################################\n",
        "\n",
        "train_data = dict()\n",
        "test_data  = dict()\n",
        "split_percentage = 10\n",
        "for k in list(namesData.keys()):\n",
        "    total_size    = len(namesData[k])\n",
        "    test_size     = round(total_size/split_percentage)\n",
        "    train_size    = total_size - test_size\n",
        "    print(k, total_size, train_size, test_size)\n",
        "    indices       = [i for i in range(total_size)]\n",
        "    random.shuffle(indices)\n",
        "    train_indices = indices[0:train_size]\n",
        "    test_indices  = indices[(train_size+1):(-1)]\n",
        "    train_data[k] = [namesData[k][i] for i in train_indices]\n",
        "    test_data[k]  = [namesData[k][i] for i in test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDTNs3zs56zG"
      },
      "source": [
        "## Define LSTM module\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUow9Vt356zI"
      },
      "source": [
        "The definition of the LSTM model follows the steps explained in the previous worksheet (6.1) closely.\n",
        "NB: we include a dropout rate (which here acts in between layers of the stacked LSTM).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BXfhNM6z56zJ"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## define LSTM\n",
        "##################################################\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, cat_embedding_size, n_cat,\n",
        "                 char_embedding_size, n_char,\n",
        "                 hidden_size, output_size, num_layers = 2, dropout = 0.1):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # category embedding\n",
        "        self.cat_embedding = nn.Embedding(n_cat, cat_embedding_size)\n",
        "        # character embedding\n",
        "        self.char_embedding = nn.Embedding(n_char, char_embedding_size)\n",
        "        # the actual LSTM\n",
        "        self.lstm = nn.LSTM(input_size  = cat_embedding_size+char_embedding_size,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers  = num_layers,\n",
        "                            batch_first = True,\n",
        "                            dropout = dropout\n",
        "                            )\n",
        "        # linear map onto weights for words\n",
        "        self.linear_map = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, category, name, hidden):\n",
        "        cat_emb  = self.cat_embedding(category)\n",
        "        char_emb = self.char_embedding(name)\n",
        "        output, (hidden, cell) = self.lstm(torch.concat([cat_emb, char_emb], dim = 1))\n",
        "        predictions = self.linear_map(output)\n",
        "        return torch.nn.functional.log_softmax(predictions, dim = 1), hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RknW2_PU56zK"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.2.1: </span></strong>\n",
        ">\n",
        "> 1. How is the category information supplied to next network? I.e.,, what is the input format and how is this information made accessible for computation at every word?\n",
        ">\n",
        "> 2. What exactly is the return value of a single forward pass?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answers: \n",
        "**Exercise 6.2.1.1**\n",
        "\n",
        "- The category information was a embedding matrix consisting of vectors of embeddings, the size of each vector is specified as cat_embedding_size.\n",
        "- The size of this embedding matrix is the same as the number of characters in the name, i.e. the embedding vector is repeated in this matrix as many times as the length of the random string name in this random training pair. \n",
        "- Then, the category embedding matrix is concatenated to the character embedding matrix and passed to the LSTM function, so that each character has access to the category information.\n",
        "\n",
        "\n",
        "**Exercise 6.2.1.2**\n",
        "\n",
        "- The output of the LSTM function consists of the last layer of category and each character embeddings, the hidden state of all layers of the last character, and the cell state of all layers of the last character.\n",
        "- Then, within the forward function, the embeddings of the last layer of category and each character is mapped to a vector of weights (predictions) of the same length as the size of all letters.\n",
        "- These predictions are then normalized by a softmax function and turned into the next-character probabilities, which is the return value of this single forward pass.\n"
      ],
      "metadata": {
        "id": "HX7YLPbmEUjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_embedding = nn.Embedding(18, 32)\n",
        "cat_embedding(torch.tensor([4, 4, 4, 4, 4]))\n"
      ],
      "metadata": {
        "id": "SAzr9dlY8gKf",
        "outputId": "88cd5ea4-f24a-4c66-fd91-273120c7eba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2508, -0.2521, -0.6208,  1.4897, -0.3861,  0.4386, -0.4958,  1.2181,\n",
              "          1.5948,  1.8021,  0.5613, -0.4163,  1.8348, -0.5390, -0.7527, -0.7715,\n",
              "          0.6915, -1.1872, -0.6388,  2.8821, -0.6194,  0.5816,  1.4587, -1.3666,\n",
              "          0.9305,  0.8972,  0.4771,  1.2385,  0.7276,  0.7727,  0.5271, -0.3466],\n",
              "        [ 0.2508, -0.2521, -0.6208,  1.4897, -0.3861,  0.4386, -0.4958,  1.2181,\n",
              "          1.5948,  1.8021,  0.5613, -0.4163,  1.8348, -0.5390, -0.7527, -0.7715,\n",
              "          0.6915, -1.1872, -0.6388,  2.8821, -0.6194,  0.5816,  1.4587, -1.3666,\n",
              "          0.9305,  0.8972,  0.4771,  1.2385,  0.7276,  0.7727,  0.5271, -0.3466],\n",
              "        [ 0.2508, -0.2521, -0.6208,  1.4897, -0.3861,  0.4386, -0.4958,  1.2181,\n",
              "          1.5948,  1.8021,  0.5613, -0.4163,  1.8348, -0.5390, -0.7527, -0.7715,\n",
              "          0.6915, -1.1872, -0.6388,  2.8821, -0.6194,  0.5816,  1.4587, -1.3666,\n",
              "          0.9305,  0.8972,  0.4771,  1.2385,  0.7276,  0.7727,  0.5271, -0.3466],\n",
              "        [ 0.2508, -0.2521, -0.6208,  1.4897, -0.3861,  0.4386, -0.4958,  1.2181,\n",
              "          1.5948,  1.8021,  0.5613, -0.4163,  1.8348, -0.5390, -0.7527, -0.7715,\n",
              "          0.6915, -1.1872, -0.6388,  2.8821, -0.6194,  0.5816,  1.4587, -1.3666,\n",
              "          0.9305,  0.8972,  0.4771,  1.2385,  0.7276,  0.7727,  0.5271, -0.3466],\n",
              "        [ 0.2508, -0.2521, -0.6208,  1.4897, -0.3861,  0.4386, -0.4958,  1.2181,\n",
              "          1.5948,  1.8021,  0.5613, -0.4163,  1.8348, -0.5390, -0.7527, -0.7715,\n",
              "          0.6915, -1.1872, -0.6388,  2.8821, -0.6194,  0.5816,  1.4587, -1.3666,\n",
              "          0.9305,  0.8972,  0.4771,  1.2385,  0.7276,  0.7727,  0.5271, -0.3466]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX48MQAU56zK"
      },
      "source": [
        "## Helper functions for training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KpJseM_56zK"
      },
      "source": [
        "Again, the following training functions are similar to what we used in sheet 5.1, but changed to handle the different representational format of the input.\n",
        "(Previous work sheet used a one-hot vector representation where we here use an index (integer) representation for each word.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oddwfj-R56zL"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## helper functions for training\n",
        "##################################################\n",
        "\n",
        "# Random item from a list\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Get a random category and random name from that category\n",
        "def randomTrainingPair():\n",
        "    category = randomChoice(categories)\n",
        "    name = randomChoice(train_data[category])\n",
        "    return category, name\n",
        "\n",
        "# get index representation of name (in the proper format)\n",
        "def getNameIndices(name):\n",
        "    indices = [SOSIndex] + [all_letters.index(c) for c in list(name)] + [EOSIndex]\n",
        "    return indices\n",
        "\n",
        "# get index representation of category (in the proper format)\n",
        "# NB: must have same length as corresponding name representation b/c\n",
        "#     each character in the sequence is concatenated with the category information\n",
        "def getCatIndices(category, name_length):\n",
        "    return torch.full((1,name_length), categories.index(category)).reshape(-1)\n",
        "\n",
        "# get random training pair in desired input format (vectors of indices)\n",
        "def randomTrainingExample():\n",
        "    category, name = randomTrainingPair()\n",
        "    name_length = len(name) + 2\n",
        "    return getCatIndices(category, name_length), torch.tensor(getNameIndices(name))\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getNameIndices(\"Sheng\")"
      ],
      "metadata": {
        "id": "4Vv48Bs9-1Dp",
        "outputId": "d9344f8d-90bb-4a94-8419-852a2b0f00dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[59, 44, 7, 4, 13, 6, 58]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getCatIndices(\"English\", 7)"
      ],
      "metadata": {
        "id": "ETUyBzzp-99e",
        "outputId": "5ebe78dc-fa32-4d29-ceba-2e2500d3e660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 9, 9, 9, 9, 9, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLGHW1ja56zL"
      },
      "source": [
        "## Single training step\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eergbByI56zM"
      },
      "source": [
        "A single training loop for a single pair of category and name considers the output predictions of the LSTM.\n",
        "The way we defined the LSTM above makes it so that the first component that is returned feeds directly in to the loss function (negative log likeihood).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD8qT9Ja56zM"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## single training pass\n",
        "##################################################\n",
        "\n",
        "def train(cat, name):\n",
        "    # get a fresh hidden layer\n",
        "    hidden = lstm.initHidden()\n",
        "    # zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # run sequence\n",
        "    predictions, hidden = lstm(cat, name, hidden)\n",
        "    # compute loss (NLLH)\n",
        "    loss = criterion(predictions[:-1], name[1:len(name)])\n",
        "    # perform backward pass\n",
        "    loss.backward()\n",
        "    # perform optimization\n",
        "    optimizer.step()\n",
        "    # return prediction and loss\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmCuxF8756zN"
      },
      "source": [
        "## Model instantiation & training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJfvRZzB56zN"
      },
      "source": [
        "The LSTM we instantiate here is rather smallish.\n",
        "It has only one layer, a hidden and cell state of size 64 and uses an embedding size of 32 for both categories and names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liZVn0Q356zN",
        "outputId": "805aa207-4fbc-4b84-8490-c8607299e5ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\n0m 6s (5000 10%) 16.7143\n0m 12s (10000 20%) 15.9386\n0m 18s (15000 30%) 15.4798\n0m 24s (20000 40%) 15.1522\n0m 30s (25000 50%) 14.9384\n0m 36s (30000 60%) 14.7415\n0m 42s (35000 70%) 14.5792\n0m 48s (40000 80%) 14.4516\n0m 54s (45000 90%) 14.3326\n1m 0s (50000 100%) 14.2348\n#+end_example"
        }
      ],
      "source": [
        "##################################################\n",
        "## actual training loop\n",
        "## (should take about 1-2 minutes)\n",
        "##################################################\n",
        "\n",
        "# instantiate model\n",
        "lstm = LSTM(cat_embedding_size  = 32,\n",
        "            n_cat               = n_categories,\n",
        "            char_embedding_size = 32,\n",
        "            n_char              = n_letters,\n",
        "            hidden_size         = 64,\n",
        "            output_size         = n_letters,\n",
        "            dropout             = 0,\n",
        "            num_layers          = 1\n",
        "            )\n",
        "# training objective\n",
        "criterion = nn.NLLLoss(reduction='sum')\n",
        "# learning rate\n",
        "learning_rate = 0.005\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "# training parameters\n",
        "n_iters = 50000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0 # will be reset every 'plot_every' iterations\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        rolling_mean = np.mean(all_losses[iter - print_every*(iter//print_every):])\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start),\n",
        "                                     iter,\n",
        "                                     iter / n_iters * 100,\n",
        "                                     rolling_mean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_f5oaRw56zP"
      },
      "source": [
        "## Plotting training performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S339op6I56zQ",
        "outputId": "23f8a715-f15e-4cd2-aeba-08217f9be2bc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "##################################################\n",
        "## monitoring loss function during training\n",
        "##################################################\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zig6Ibeh56zQ"
      },
      "source": [
        "## Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5z3q4zP56zR"
      },
      "source": [
        "Extraction of surprisal of a single item and computation of average surprisal for test and training set is largely parallel to the case of sheet 5.1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4pCtp5i56zR"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## evaluation\n",
        "##################################################\n",
        "\n",
        "def get_surprisal_item(category, name):\n",
        "    name      = torch.tensor(getNameIndices(name))\n",
        "    cat       = getCatIndices(category,len(name))\n",
        "    hidden    = lstm.initHidden()\n",
        "    prediction, hidden = lstm(cat, name, hidden)\n",
        "    nll       = criterion(prediction[:-1], name[1:len(name)])\n",
        "    return(nll.item())\n",
        "\n",
        "def get_surprisal_dataset(data):\n",
        "    surprisl_dict = dict()\n",
        "    surp_avg_dict = dict()\n",
        "    perplxty_dict = dict()\n",
        "    for category in list(data.keys()):\n",
        "        surprisl = 0\n",
        "        surp_avg = 0\n",
        "        perplxty = 0\n",
        "        # training\n",
        "        for name in data[category]:\n",
        "            item_surpr = get_surprisal_item(category, name)\n",
        "            surprisl  += item_surpr\n",
        "            surp_avg  += item_surpr / len(name)\n",
        "            perplxty  += item_surpr ** (-1 / len(name))\n",
        "        n_items = len(data[category])\n",
        "\n",
        "        surprisl_dict[category] = (surprisl /n_items)\n",
        "        surp_avg_dict[category] = (surp_avg / n_items)\n",
        "        perplxty_dict[category] = (perplxty / n_items)\n",
        "\n",
        "    return(surprisl_dict, surp_avg_dict, perplxty_dict)\n",
        "\n",
        "def makeDF(surp_dict):\n",
        "    p = pandas.DataFrame.from_dict(surp_dict)\n",
        "    p = p.transpose()\n",
        "    p.columns = [\"surprisal\", \"surp_scaled\", \"perplexity\"]\n",
        "    return(p)\n",
        "\n",
        "surprisal_test  = makeDF(get_surprisal_dataset(test_data))\n",
        "surprisal_train = makeDF(get_surprisal_dataset(train_data))\n",
        "\n",
        "print(\"\\nmean surprisal (test):\", np.mean(surprisal_test[\"surprisal\"]))\n",
        "print(\"\\nmean surprisal (train):\", np.mean(surprisal_train[\"surprisal\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFN1Vmtj56zR"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.2.2: Interpret the evaluation metrics </span></strong>\n",
        ">\n",
        "> 1. What do you conclude from these two numbers? Is there a chance that the model overfitted the training data?\n",
        ">\n",
        "> 2. What do you conclude about the performance of the RNN (from sheet 5.1) and the current LSTM implementation? Which model is better?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYAHCgaK56zR"
      },
      "source": [
        "## Exploring model predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP7b3gqE56zR"
      },
      "source": [
        "Now the fun part starts!\n",
        "Let&rsquo;s see how the generations of the LSTM look like.\n",
        "Notice that there is a flag for the kind of decoding strategy to be used.\n",
        "Currently, there are two decoding strategies (but see exercise below).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2obwJj4f56zS"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## prediction function\n",
        "##################################################\n",
        "\n",
        "max_length = 20\n",
        "\n",
        "# make a prediction based on given sequence\n",
        "def predict(category, initial_sequence, decode_strat = \"greedy\"):\n",
        "\n",
        "    if len(initial_sequence) >= max_length:\n",
        "        return(initial_sequence)\n",
        "\n",
        "    name      = torch.tensor(getNameIndices(initial_sequence))[:-1]\n",
        "    cat       = getCatIndices(category,len(name))\n",
        "    hidden    = lstm.initHidden()\n",
        "\n",
        "    generation = initial_sequence\n",
        "\n",
        "    output, hidden = lstm(cat, name, hidden)\n",
        "    next_word_pred = output[-1]\n",
        "\n",
        "    if decode_strat == \"pure\":\n",
        "        sample_index = torch.multinomial(input = torch.exp(next_word_pred),\n",
        "                                         num_samples = 1)\n",
        "        pass\n",
        "    else:\n",
        "        topv, topi = next_word_pred.topk(1)\n",
        "        sample_index = topi[0].item()\n",
        "\n",
        "    if sample_index == EOSIndex:\n",
        "        return(generation)\n",
        "    else:\n",
        "        generation += all_letters[sample_index]\n",
        "\n",
        "    return(predict(category, generation))\n",
        "\n",
        "print(predict(\"German\", \"\", decode_strat = \"greedy\"))\n",
        "print(predict(\"German\", \"\", decode_strat = \"pure\"))\n",
        "print(predict(\"German\", \"\", decode_strat = \"pure\"))\n",
        "print(predict(\"German\", \"\", decode_strat = \"pure\"))\n",
        "\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"greedy\"))\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"pure\"))\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"pure\"))\n",
        "print(predict(\"Japanese\", \"\", decode_strat = \"pure\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItbeOvK056zS"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.2.3: Predictions under different decoding schemes </span></strong>\n",
        ">\n",
        "> 0. [Just for yourself] Play around with the prediction function. Are you content with the quality of the predictions? Is the model performing better than the previous RNN in your perception?\n",
        ">\n",
        "> 1. Extend the function &rsquo;predict&rsquo; by implementing three additional decoding schemes: top-k, softmax and top-p. Write the function in such a way that the decoding strategy can be chosen by the user with a mnemonic string (like already don for the &ldquo;pure&rdquo; decoding strategy).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX_WRgDj56zT"
      },
      "source": [
        "## [Excursion] Class predictions from the generation model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqTU2Yy_56zT"
      },
      "source": [
        "In the previous sheet 5.1 we looked at a way of using the string generation probabilities for categorization.\n",
        "Here is a function that does that, too, but now for the LSTM model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFQrsCn956zT"
      },
      "outputs": [],
      "source": [
        "def infer_category(name):\n",
        "    probs = torch.tensor([torch.exp(-torch.tensor(get_surprisal_item(c, name))) for c in categories])\n",
        "    probs = probs/torch.sum(probs)\n",
        "    vals, cats = probs.topk(3)\n",
        "    print(\"Top 3 guesses for \", name, \":\\n\")\n",
        "    for i in range(len(cats)):\n",
        "        print(\"%12s: %.5f\" %\n",
        "              (categories[cats[i]], vals[i].detach().numpy() ))\n",
        "\n",
        "infer_category(\"Smith\")\n",
        "infer_category(\"Miller\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiaRm5L456zU"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.2.4: Reflect on derived category predictions </span></strong>\n",
        ">\n",
        "> 1. [This is a bonus exercise; optional!] Check out the model&rsquo;s predictions for &ldquo;Smith&rdquo; and &ldquo;Miller&rdquo;. Is this what you would expect of a categorization function? Why? Why not? Can you explain why the this &ldquo;derived categorization model&rdquo; makes these predictions?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}