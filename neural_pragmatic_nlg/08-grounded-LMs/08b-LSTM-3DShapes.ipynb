{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wHnq54bM35Y"
      },
      "source": [
        "Sheet 8.1: An LSTM-based image captioner for the annotated 3D-Shapes data set\n",
        "=============================================================================\n",
        "\n",
        "**Author:** Michael Franke & Polina Tsvilodub\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCiJql7fM35j"
      },
      "source": [
        "In order to see how a custom-built neural image captioning (NIC) system can be implemented and used for inference, we look here at a relatively simple LSTM-based image captioner.\n",
        "To shortcut the time it takes for training this model, we use downloadable weights from a trained version of this model.\n",
        "The goal is to see the architecture in full detail and to get a feeling for the way in which images and text data is handled during inference.\n",
        "Most importantly, however, we will want to get a feeling for how good the model&rsquo;s predictions are, at least intuitively.\n",
        "\n",
        "This tutorial uses a synthetic data set of annotations for the [3D Shapes data set](https://github.com/deepmind/3d-shapes), which we will refer to as &ldquo;annotated 3D Shapes data set&rdquo; or &ldquo;A3DS&rdquo; for short.\n",
        "Other image-captioning data sets (like MS-Coco) contain heterogeneous pictures and often only a small number of captions per picture, making it less clear whether it is the NIC&rsquo;s fault or the potentially poor quality of the data set that causes generations to be intuitively inadequate (garbled, untrue, over- or underspecified &#x2026;).\n",
        "In contrast, using the A3DS data set, makes it easier to judge, on intuitive grounds, whether generated captions are any good.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcoWw26VM35m"
      },
      "source": [
        "## Credits and origin of material\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy8atuEFM35n"
      },
      "source": [
        "The material presented in this tutorial is in large part based on work conducted by Polina Tsvilodub for her 2022 MSc thesis &ldquo;Language Drift of Multi-Agent Communication Systems in Reference Games&rdquo;.\n",
        "In particular, the annotated 3D Shapes data set and the LSTM-based architecture stems from this thesis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NunOemjuM35o"
      },
      "source": [
        "## Packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1cQVYIMM35q"
      },
      "source": [
        "On top of the usual suspects, we will use the &rsquo;Image&rsquo; and &rsquo;torchvision&rsquo; package to process image data.\n",
        "We need package &rsquo;pickle&rsquo; to load image data and pre-trained model weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/michael-franke/npNLG/raw/main/neural_pragmatic_nlg/data/A3DS/A3DS.zip\n",
        "!unzip A3DS.zip"
      ],
      "metadata": {
        "id": "e8Cw2FIKM7FT",
        "outputId": "40f3e51f-ad68-4f70-9b29-92e970759210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-11 09:41:29--  https://github.com/michael-franke/npNLG/raw/main/neural_pragmatic_nlg/data/A3DS/A3DS.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/michael-franke/npNLG/main/neural_pragmatic_nlg/data/A3DS/A3DS.zip [following]\n",
            "--2023-01-11 09:41:29--  https://raw.githubusercontent.com/michael-franke/npNLG/main/neural_pragmatic_nlg/data/A3DS/A3DS.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31636302 (30M) [application/zip]\n",
            "Saving to: ‘A3DS.zip’\n",
            "\n",
            "\rA3DS.zip              0%[                    ]       0  --.-KB/s               \rA3DS.zip            100%[===================>]  30.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-01-11 09:41:29 (246 MB/s) - ‘A3DS.zip’ saved [31636302/31636302]\n",
            "\n",
            "Archive:  A3DS.zip\n",
            "   creating: A3DS/\n",
            "  inflating: __MACOSX/._A3DS         \n",
            "  inflating: A3DS/sandbox_3Dshapes_1000.pkl  \n",
            "  inflating: __MACOSX/A3DS/._sandbox_3Dshapes_1000.pkl  \n",
            "  inflating: A3DS/sandbox_3Dshapes_resnet50_features_1000.pt  \n",
            "  inflating: __MACOSX/A3DS/._sandbox_3Dshapes_resnet50_features_1000.pt  \n",
            "  inflating: A3DS/vocab.pkl          \n",
            "  inflating: __MACOSX/A3DS/._vocab.pkl  \n",
            "  inflating: A3DS/pretrained_decoder_3dshapes.pkl  \n",
            "  inflating: __MACOSX/A3DS/._pretrained_decoder_3dshapes.pkl  \n",
            "  inflating: A3DS/sandbox_IDs_3dshapes_1000.txt  \n",
            "  inflating: __MACOSX/A3DS/._sandbox_IDs_3dshapes_1000.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnwOpFdVM35s"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## import packages\n",
        "##################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from random import shuffle\n",
        "# from tqdm import tqdm\n",
        "import pickle\n",
        "# import 5py\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKTjZH_LM35w"
      },
      "source": [
        "## The A3DS data set\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2DftR4LM35z"
      },
      "source": [
        "The following code gives us PyTorch &rsquo;Dataset&rsquo; and &rsquo;DataLoader&rsquo; objects, with which to handle a 1k-subset of images and annotations from the A3DS data set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMdP4ONfM350"
      },
      "source": [
        "### The &rsquo;Dataset&rsquo; object\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jui07UvFM351"
      },
      "source": [
        "Here is the definition of the &rsquo;Dataset&rsquo; object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sDldp19M352"
      },
      "outputs": [],
      "source": [
        "class A3DS(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for loading the dataset of images and captions from the 3dshapes dataset.\n",
        "\n",
        "    Arguments:\n",
        "    ---------\n",
        "    num_labels: int\n",
        "        Number of distinct captions to sample for each image. Relevant for using the dataloader for training models.\n",
        "    labels_type: str\n",
        "        \"long\" or \"short\". Indicates whether long or short captions should be used.\n",
        "    run_inference: bool\n",
        "        Flag indicating whether this dataset will be used for performing inference with a trained image captioner.\n",
        "    batch_size: int\n",
        "        Batch size. Has to be 1 in order to save the example image-caption pairs.\n",
        "    vocab_file: str\n",
        "        Name of vocab file.\n",
        "    start_token: str\n",
        "        Start token.\n",
        "    end_token: str\n",
        "        End token.\n",
        "    unk_token: str\n",
        "        Token to be used when encoding unknown tokens.\n",
        "    pad_token: str\n",
        "        Pad token to be used for padding captions tp max_sequence_length.\n",
        "    max_sequence_length: int\n",
        "        Length to which all captions are padded / truncated.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            path=\"A3DS\",\n",
        "            num_labels=1, # number of ground truth labels to retrieve per image\n",
        "            labels_type=\"long\", # alternative: short\n",
        "            run_inference=False, # depending on this flag, check presence of model weights\n",
        "            batch_size=1,\n",
        "            vocab_file=\"vocab.pkl\",\n",
        "            start_token=\"START\",  # might be unnecessary since vocab file is fixed anyways\n",
        "            end_token=\"END\",\n",
        "            unk_token=\"UNK\",\n",
        "            pad_token=\"PAD\",\n",
        "            max_sequence_length=26, # important for padding length\n",
        "        ):\n",
        "\n",
        "        # check vocab file exists\n",
        "        assert os.path.exists(os.path.join(path, vocab_file)), \"Make sure the vocab file exists in the directory passed to the dataloader (see README)\"\n",
        "\n",
        "        # check if image file exists\n",
        "        assert (os.path.exists(os.path.join(path, \"sandbox_3Dshapes_1000.pkl\")) and os.path.join(path, \"sandbox_3Dshapes_resnet50_features_1000.pt\")), \"Make sure the sandbox dataset exists in the directory passed to the dataloader (see README)\"\n",
        "\n",
        "        if labels_type == \"long\":\n",
        "            assert num_labels <= 20, \"Maximally 20 distinct image-long caption pairs can be created for one image\"\n",
        "        else:\n",
        "            assert num_labels <= 27, \"Maximally 27 distinct image-short caption pairs can be created for one image\"\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        with open(os.path.join(path, vocab_file), \"rb\") as vf:\n",
        "            self.vocab = pickle.load(vf)\n",
        "\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.unk_token = unk_token\n",
        "        self.pad_token = pad_token\n",
        "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "        self.embedded_imgs = torch.load(os.path.join(path, \"sandbox_3Dshapes_resnet50_features_1000.pt\"))\n",
        "        with open(os.path.join(path, \"sandbox_3Dshapes_1000.pkl\"), \"rb\") as f:\n",
        "            self.sandbox_file = pickle.load(f)\n",
        "            self.images = self.sandbox_file[\"images\"]\n",
        "            self.numeric_labels = self.sandbox_file[\"labels_numeric\"]\n",
        "            self.labels_long = self.sandbox_file[\"labels_long\"]\n",
        "            self.labels_short = self.sandbox_file[\"labels_short\"]\n",
        "\n",
        "        if labels_type == \"long\":\n",
        "            labels_ids_flat = [list(np.random.choice(range(len(self.labels_long[0])), num_labels, replace=False)) for i in range(len(self.images))]\n",
        "            self.labels_flat = [self.labels_long[i][l] for i, sublst in enumerate(labels_ids_flat) for l in sublst]\n",
        "            self.img_ids_flat = [id for id in range(len(self.images)) for i in range(num_labels)]\n",
        "        else:\n",
        "            labels_ids_flat = [list(np.random.choice(range(len(self.labels_short[0])), num_labels, replace=False)) for i in range(len(self.images))]\n",
        "            self.labels_flat = [self.labels_short[i][l] for i, sublst in enumerate(labels_ids_flat) for l in sublst]\n",
        "            self.img_ids_flat = [i for id in range(len(self.images)) for id in range(num_labels)]\n",
        "\n",
        "        # print(\"len labels ids flat \", len(labels_ids_flat))\n",
        "        # print(\"len labels flat \", len(self.labels_flat), self.labels_flat[:5])\n",
        "        # print(\"len image ids flat \", len(self.img_ids_flat), self.img_ids_flat[:5])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns length of dataset.\n",
        "        \"\"\"\n",
        "        return len(self.img_ids_flat)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Iterator over the dataset.\n",
        "\n",
        "        Arguments:\n",
        "        ---------\n",
        "        idx: int\n",
        "            Index for accessing the flat image-caption pairs.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "        target_img: np.ndarray (64,64,3)\n",
        "            Original image.\n",
        "        target_features: torch.Tensor(2048,)\n",
        "            ResNet features of the image.\n",
        "        target_lbl: str\n",
        "            String caption.\n",
        "        numeric_lbl: np.ndarray (6,)\n",
        "            Original numeric image annotation.\n",
        "        target_caption: torch.Tensor(batch_size, 25)\n",
        "            Encoded caption.\n",
        "        \"\"\"\n",
        "        # access raw image corresponding to the index in the entire dataset\n",
        "        target_img = self.images[self.img_ids_flat[idx]]\n",
        "        # access caption\n",
        "        target_lbl = self.labels_flat[idx]\n",
        "        # access original numeric annotation of the image\n",
        "        numeric_lbl = self.numeric_labels[self.img_ids_flat[idx]]\n",
        "        # cast type\n",
        "        target_img = np.asarray(target_img).astype('uint8')\n",
        "        # retrieve ResNet features, accessed through original image ID\n",
        "        target_features = self.embedded_imgs[self.img_ids_flat[idx]]\n",
        "        # tokenize label\n",
        "        tokens = self.tokenizer(str(target_lbl).lower().replace(\"-\", \" \"))\n",
        "        # Convert caption to tensor of word ids, append start and end tokens.\n",
        "        target_caption = self.tokenize_caption(tokens)\n",
        "        # convert to tensor\n",
        "        target_caption = torch.Tensor(target_caption).long()\n",
        "\n",
        "        return target_img, target_features, target_lbl, numeric_lbl, target_caption\n",
        "\n",
        "    def tokenize_caption(self, label):\n",
        "        \"\"\"\n",
        "        Helper for converting list of tokens into list of token IDs.\n",
        "        Expects tokenized caption as input.\n",
        "\n",
        "        Arguments:\n",
        "        --------\n",
        "        label: list\n",
        "            Tokenized caption.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "        tokens: list\n",
        "            List of token IDs, prepended with start, end, padded to max length.\n",
        "        \"\"\"\n",
        "        label = label[:self.max_sequence_length]\n",
        "        tokens = [self.vocab[\"word2idx\"][self.start_token]]\n",
        "        for t in label:\n",
        "            try:\n",
        "                tokens.append(self.vocab[\"word2idx\"][t])\n",
        "            except:\n",
        "                tokens.append(self.vocab[\"word2idx\"][self.unk_token])\n",
        "        tokens.append(self.vocab[\"word2idx\"][self.end_token])\n",
        "        # pad\n",
        "        while len(tokens) < self.max_sequence_length:\n",
        "            tokens.append(self.vocab[\"word2idx\"][self.pad_token])\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def get_labels_for_image(self, id, caption_type=\"long\"):\n",
        "        \"\"\"\n",
        "        Helper for getting all annotations for a given image id.\n",
        "\n",
        "        Arguments:\n",
        "        ---------\n",
        "        id: int\n",
        "            Index of image caption pair containing the image\n",
        "            for which the full list of captions should be returned.\n",
        "        caption_type: str\n",
        "            \"long\" or \"short\". Indicates type of captions to provide.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "            List of all captions for given image.\n",
        "        \"\"\"\n",
        "        if caption_type == \"long\":\n",
        "            return self.labels_long[self.img_ids_flat[id]]\n",
        "        else:\n",
        "            return self.labels_short[self.img_ids_flat[id]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVvhUHsWM355"
      },
      "source": [
        "Lets instantiate the &rsquo;Dataset&rsquo; object and explore the structure of the A3DS data.\n",
        "Notice that there are a 1000 items in this subset of the A3DS data set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFLccExgM357",
        "outputId": "4ae7f51d-988e-4405-cc27-5d53f5845335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ],
      "source": [
        "A3DS_dataset = A3DS()\n",
        "print(A3DS_dataset.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwzyRolJM36A"
      },
      "source": [
        "Let&rsquo;s get a single item by some ID, here taking the first item.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC5Bibl5M36B"
      },
      "outputs": [],
      "source": [
        "itemID=0\n",
        "image, target_features, caption_text, numeric_lbl, caption_indx = A3DS_dataset.__getitem__(itemID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tTpCgQkM36C"
      },
      "source": [
        "Each item is a tuple with 5 pieces of information.\n",
        "For our purposes, the most important ones are in slot 0 (the image information) and in slot 2 (the caption as a text).\n",
        "\n",
        "Let&rsquo;s have a look at the image, which is stored as a tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZBcKnjtM36D",
        "outputId": "30bd2760-73a0-4b2c-d071-54260cbfbb9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[153 226 249]\n",
            "  [153 226 249]\n",
            "  [153 226 249]\n",
            "  ...\n",
            "  [153 226 249]\n",
            "  [153 226 249]\n",
            "  [153 226 249]]\n",
            "\n",
            " [[153 226 249]\n",
            "  [153 226 249]\n",
            "  [153 226 249]\n",
            "  ...\n",
            "  [153 226 249]\n",
            "  [153 226 249]\n",
            "  [153 226 249]]\n",
            "\n",
            " [[153 226 249]\n",
            "  [153 226 249]\n",
            "  [153 226 249]\n",
            "  ...\n",
            "  [153 226 249]\n",
            "  [153 226 249]\n",
            "  [153 226 249]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[254   0   0]\n",
            "  [254   0   0]\n",
            "  [253   0   0]\n",
            "  ...\n",
            "  [214   0   0]\n",
            "  [216   0   0]\n",
            "  [219   0   0]]\n",
            "\n",
            " [[251   0   0]\n",
            "  [246   0   0]\n",
            "  [250   0   0]\n",
            "  ...\n",
            "  [220   0   0]\n",
            "  [215   0   0]\n",
            "  [212   0   0]]\n",
            "\n",
            " [[255   0   0]\n",
            "  [248   0   0]\n",
            "  [243   0   0]\n",
            "  ...\n",
            "  [219   0   0]\n",
            "  [219   0   0]\n",
            "  [217   0   0]]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# picture\n",
        "print(image)\n",
        "\n",
        "# plot image\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTqZqm9SM36E"
      },
      "source": [
        "And here is a caption that goes with this picture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiSgUN3pM36F",
        "outputId": "2cb7ca49-b247-4d85-daa9-8b70fd88d963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a small orange cylinder located in the left corner in front of a purple wall on red floor\n"
          ]
        }
      ],
      "source": [
        "# ground-truth caption\n",
        "print(caption_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92fyQ-RvM36F"
      },
      "source": [
        "There are actually long and short captions for each image.\n",
        "We have created an instance of the data set with one random long caption per image.\n",
        "We can inspect the full list of short captions like so:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pCTmO7AM36G",
        "outputId": "2e1e01f9-b68c-4776-f099-8d817ab4b615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there is a small cylinder\n",
            "there is a orange cylinder\n",
            "there is a cylinder in the left corner\n",
            "there is a cylinder in front of a purple wall\n",
            "there is a cylinder on red floor\n",
            "there is a small cylinder in the left corner\n",
            "there is a small cylinder in front of a purple wall\n",
            "there is a small cylinder on red floor\n",
            "there is a orange cylinder in the left corner\n",
            "there is a orange cylinder in front of a purple wall\n",
            "there is a orange cylinder on red floor\n",
            "a small cylinder\n",
            "a orange cylinder\n",
            "a cylinder in the left corner\n",
            "a cylinder in front of a purple wall\n",
            "a cylinder on red floor\n",
            "a small cylinder in the left corner\n",
            "a small cylinder in front of a purple wall\n",
            "a small cylinder on red floor\n",
            "a orange cylinder in the left corner\n",
            "a orange cylinder in front of a purple wall\n",
            "a orange cylinder on red floor\n",
            "the cylinder is in the left corner\n",
            "the cylinder is in front of a purple wall\n",
            "the cylinder is on red floor\n",
            "the cylinder is small\n",
            "the cylinder is orange\n"
          ]
        }
      ],
      "source": [
        "# Retrieve all short-captions for the image ID:\n",
        "all_short_caps = A3DS_dataset.get_labels_for_image(itemID, caption_type='short')\n",
        "for c in all_short_caps:\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hnLl9yFM36H"
      },
      "source": [
        "And similarly for the long captions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WN0FweEM36H",
        "outputId": "c6d9548b-56b1-4e92-d704-e58cd7aa4404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a small orange cylinder in the left corner in front of a purple wall on red floor\n",
            "a small orange cylinder in the left corner on red floor in front of a purple wall\n",
            "a small orange cylinder on red floor in the left corner in front of a purple wall\n",
            "a small orange cylinder on red floor in front of a purple wall in the left corner\n",
            "the picture shows a small orange cylinder in the left corner in front of a purple wall on red floor\n",
            "the picture shows a small orange cylinder in the left corner on red floor in front of a purple wall\n",
            "the picture shows a small orange cylinder on red floor in the left corner in front of a purple wall\n",
            "the picture shows a small orange cylinder on red floor in front of a purple wall in the left corner\n",
            "a small orange cylinder located in the left corner in front of a purple wall on red floor\n",
            "a small orange cylinder located in the left corner on red floor in front of a purple\n",
            "a small orange cylinder located on red floor in the left corner in front of a purple wall\n",
            "a small orange cylinder located on red floor in front of a purple wall in the left corner\n",
            "the small cylinder in the left corner in front of a purple wall on red floor is orange\n",
            "the small cylinder in the left corner on red floor in front of a purple wall is orange\n",
            "the small cylinder on red floor in the left corner in front of a purple wall is orange\n",
            "the small cylinder on red floor in front of a purple wall in the left corner is orange\n",
            "the orange cylinder in the left corner in front of a purple wall on red floor is small\n",
            "the orange cylinder in the left corner on red floor in front of a purple wall is small\n",
            "the orange cylinder on red floor in the left corner in front of a purple wall is small\n",
            "the orange cylinder on red floor in front of a purple wall in the left corner is small\n"
          ]
        }
      ],
      "source": [
        "# Retrieve all long-captions for the image ID:\n",
        "\n",
        "all_long_caps = A3DS_dataset.get_labels_for_image(itemID, caption_type='long')\n",
        "for c in all_long_caps:\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDvlGlJLM36I"
      },
      "source": [
        "Finally, let&rsquo;s also have a look at the vocabulary for this A3DS data set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHjP87jgM36J",
        "outputId": "9077e0be-6ea3-47a2-de28-62abc549edee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB:  dict_keys(['START', 'END', 'UNK', 'PAD', 'a', 'tiny', 'red', 'block', 'in', 'the', 'right', 'corner', 'front', 'of', 'wall', 'on', 'floor', 'picture', 'shows', 'standing', 'is', 'close', 'to', 'side', 'near', 'middle', 'nearly', 'left', 'cylinder', 'ball', 'pill', 'small', 'medium', 'sized', 'big', 'large', 'huge', 'giant', 'orange', 'yellow', 'light', 'green', 'dark', 'cyan', 'blue', 'purple', 'pink'])\n",
            "VOCAB SIZE:  47\n"
          ]
        }
      ],
      "source": [
        "vocab = A3DS_dataset.vocab[\"word2idx\"].keys()\n",
        "print(\"VOCAB: \", vocab)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(\"VOCAB SIZE: \", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnye08QGM36L"
      },
      "source": [
        "We see that this vocabulary is actually pretty small.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "047TruYgM36L"
      },
      "source": [
        "### Creating a &rsquo;DataLoader&rsquo;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhVHzmaTM36M"
      },
      "source": [
        "Let&rsquo;s create a &rsquo;DataLoader&rsquo; for batches of a specified size, using a random shuffle of the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOtIlNV2M36N"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "A3DS_data_loader = torch.utils.data.DataLoader(\n",
        "    dataset    = A3DS_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle    = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnORcGq9M36N"
      },
      "source": [
        "## The (pre-trained) LSTM NIC\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637k0QfzM36O"
      },
      "source": [
        "Definition of the LSTM-based neural image captioner as an instance of PyTorch&rsquo;s &rsquo;nn.Module&rsquo;:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlYepCrHM36P"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, visual_embed_size, batch_size=1, num_layers=1):\n",
        "        \"\"\"\n",
        "        Initialize the language module consisting of a one-layer LSTM and\n",
        "        trainable embeddings. The image embeddings (both target and distractor!)\n",
        "        are used as additional context at every step of the training\n",
        "        (prepended to each word embedding).\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "            embed_size: int\n",
        "                Dimensionality of trainable embeddings.\n",
        "            hidden_size: int\n",
        "                Hidden/ cell state dimensionality of the LSTM.\n",
        "            vocab_size: int\n",
        "                Length of vocabulary.\n",
        "            visual_embed_size: int\n",
        "                Dimensionality of each image embedding to be appended at each time step as additional context.\n",
        "            batch_size: int\n",
        "                Batch size.\n",
        "            num_layers: int\n",
        "                Number of LSTM layers.\n",
        "        \"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size= embed_size\n",
        "        self.vocabulary_size = vocab_size\n",
        "        self.visual_embed_size = visual_embed_size\n",
        "        # embedding layer\n",
        "        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size)\n",
        "        # layer projecting ResNet features of a single image to desired size\n",
        "        self.project = nn.Linear(2048, self.visual_embed_size)\n",
        "\n",
        "      # LSTM takes as input the word embedding with prepended embeddings of the two images at each time step\n",
        "        # note that the batch dimension comes first\n",
        "        self.lstm = nn.LSTM(self.embed_size + 2*self.visual_embed_size, self.hidden_size , self.num_layers, batch_first=True)\n",
        "        # transforming last lstm hidden state to scores over vocabulary\n",
        "        self.linear = nn.Linear(hidden_size, self.vocabulary_size)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        # initial hidden state of the lstm\n",
        "        self.hidden = self.init_hidden(self.batch_size)\n",
        "\n",
        "        # initialization of the layers\n",
        "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        \"\"\"\n",
        "        At the start of training, we need to initialize a hidden state;\n",
        "        Defines a hidden state with all zeroes\n",
        "        The axes are (num_layers, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        # if torch.backends.mps.is_available():\n",
        "        #     device = torch.device(\"mps\")\n",
        "        # elif torch.cuda.is_available():\n",
        "        #     device = torch.device(\"cuda\")\n",
        "        # else:\n",
        "        #     device = torch.device(\"cpu\")\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "        return (torch.zeros((1, batch_size, self.hidden_size), device=device), \\\n",
        "                torch.zeros((1, batch_size, self.hidden_size), device=device))\n",
        "\n",
        "    def forward(self, features, captions, prev_hidden):\n",
        "        \"\"\"\n",
        "        Perform forward step through the LSTM.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "            features: torch.tensor(batch_size, 2, embed_size)\n",
        "                Embeddings of images, target and distractor concatenated in this order.\n",
        "            captions: torch.tensor(batch_size, caption_length)\n",
        "                Lists of indices representing tokens of each caption.\n",
        "            prev_hidden: (torch.tensor(num_layers, batch_size, hidden_size), torch.tensor(num_layers, batch_size, hidden_size))\n",
        "                Tuple containing previous hidden and cell states of the LSTM.\n",
        "        Returns:\n",
        "        ------\n",
        "            outputs: torch.tensor(batch_size, caption_length, embedding_dim)\n",
        "                Scores over vocabulary for each token in each caption.\n",
        "            hidden_state: (torch.tensor(num_layers, batch_size, hidden_size), torch.tensor(num_layers, batch_size, hidden_size))\n",
        "                Tuple containing new hidden and cell state of the LSTM.\n",
        "        \"\"\"\n",
        "\n",
        "        # features of shape (batch_size, 2, 2048)\n",
        "        image_emb = self.project(features) # image_emb should have shape (batch_size, 2, 512)\n",
        "        # concatenate target and distractor embeddings\n",
        "        img_features = torch.cat((image_emb[:, 0, :], image_emb[:, 1, :]), dim=-1).unsqueeze(1)\n",
        "        embeddings = self.embed(captions)\n",
        "        # repeat image features such that they can be prepended to each token\n",
        "        img_features_reps = img_features.repeat(1, embeddings.shape[1], 1)\n",
        "        # PREpend the feature embedding as additional context as first token, assume there is no END token\n",
        "        embeddings = torch.cat((img_features_reps, embeddings), dim=-1)\n",
        "        out, hidden_state = self.lstm(embeddings, prev_hidden)\n",
        "        # project LSTM predictions on to vocab\n",
        "        outputs = self.linear(out) # prediction shape is (batch_size, max_sequence_length, vocab_size)\n",
        "        # print(\"outputs shape in forward \", outputs.shape)\n",
        "        return outputs, hidden_state\n",
        "\n",
        "    def log_prob_helper(self, logits, values):\n",
        "        \"\"\"\n",
        "        Helper function for scoring the sampled token,\n",
        "        because it is not implemented for MPS yet.\n",
        "        Just duplicates source code from PyTorch.\n",
        "        \"\"\"\n",
        "        values = values.long().unsqueeze(-1)\n",
        "        values, log_pmf = torch.broadcast_tensors(values, logits)\n",
        "        values = values[..., :1]\n",
        "        return log_pmf.gather(-1, values).squeeze(-1)\n",
        "\n",
        "    def sample(self, inputs, max_sequence_length):\n",
        "        \"\"\"\n",
        "        Function for sampling a caption during functional (reference game) training.\n",
        "        Implements greedy sampling. Sampling stops when END token is sampled or when max_sequence_length is reached.\n",
        "        Also returns the log probabilities of the action (the sampled caption) for REINFORCE.\n",
        "\n",
        "        Args:\n",
        "        ----\n",
        "            inputs: torch.tensor(1, 1, embed_size)\n",
        "                pre-processed image tensor.\n",
        "            max_sequence_length: int\n",
        "                Max length of sequence which the nodel should generate.\n",
        "        Returns:\n",
        "        ------\n",
        "            output: list\n",
        "                predicted sentence (list of tensor ids).\n",
        "            log_probs: torch.Tensor\n",
        "                log probabilities of the generated tokens (up to and including first END token)\n",
        "            raw_outputs: torch.Tensor\n",
        "                Raw logits for each prediction timestep.\n",
        "            entropies: torch.Tesnor\n",
        "                Entropies at each generation timestep.\n",
        "        \"\"\"\n",
        "\n",
        "        # placeholders for output\n",
        "        output = []\n",
        "        raw_outputs = [] # for structural loss computation\n",
        "        log_probs = []\n",
        "        entropies = []\n",
        "        batch_size = inputs.shape[0]\n",
        "        softmax = nn.Softmax(dim=-1)\n",
        "        init_hiddens = self.init_hidden(batch_size)\n",
        "\n",
        "        # if torch.backends.mps.is_available():\n",
        "        #     device = torch.device(\"mps\")\n",
        "        # elif torch.cuda.is_available():\n",
        "        #     device = torch.device(\"cuda\")\n",
        "        # else:\n",
        "        #     device = torch.device(\"cpu\")\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "        #### start sampling ####\n",
        "        for i in range(max_sequence_length):\n",
        "            if i == 0:\n",
        "                cat_samples = torch.tensor([0]).repeat(batch_size, 1)\n",
        "                hidden_state = init_hiddens\n",
        "\n",
        "            cat_samples = cat_samples.to(device)\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            out, hidden_state = self.forward(inputs, cat_samples, hidden_state)\n",
        "            \n",
        "            # get and save probabilities and save raw outputs\n",
        "            raw_outputs.extend(out)\n",
        "            probs = softmax(out)\n",
        "\n",
        "            max_probs, cat_samples = torch.max(probs, dim = -1)\n",
        "            log_p = torch.log(max_probs)\n",
        "            entropy = -log_p * max_probs\n",
        "\n",
        "            top5_probs, top5_inds = torch.topk(probs, 5, dim=-1)\n",
        "\n",
        "            entropies.append(entropy)\n",
        "            output.append(cat_samples)\n",
        "            # cat_samples = torch.cat((cat_samples, cat_samples), dim=-1)\n",
        "            # print(\"Cat samples \", cat_samples)\n",
        "            log_probs.append(log_p)\n",
        "\n",
        "\n",
        "        output = torch.stack(output, dim=-1).squeeze(1)\n",
        "        # stack\n",
        "        log_probs = torch.stack(log_probs, dim=1).squeeze(-1)\n",
        "        entropies = torch.stack(entropies, dim=1).squeeze(-1)\n",
        "\n",
        "        ####\n",
        "        # get effective log prob and entropy values - the ones up to (including) END (word2idx = 1)\n",
        "        # mask positions after END - both entropy and log P should be 0 at those positions\n",
        "        end_mask = output.size(-1) - (torch.eq(output, 1).to(torch.int64).cumsum(dim=1) > 0).sum(dim=-1)\n",
        "        # include the END token\n",
        "        end_inds = end_mask.add_(1).clamp_(max=output.size(-1)) # shape: (batch_size,)\n",
        "        for pos, i in enumerate(end_inds):\n",
        "            # zero out log Ps and entropies\n",
        "            log_probs[pos, i:] = 0\n",
        "            entropies[pos, i:] = 0\n",
        "        ####\n",
        "\n",
        "        raw_outputs = torch.stack(raw_outputs, dim=1).view(batch_size, -1, self.vocabulary_size)\n",
        "        return output, log_probs, raw_outputs, entropies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGvq0rIrM36R"
      },
      "source": [
        "Instantiate the module (with appropriate specs), load weights and instantiate weights with pre-trained weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YukcqZc_M36S",
        "outputId": "ae05f18d-641a-4064-ec5d-a87c539a863d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# decoder configs\n",
        "embed_size = 512\n",
        "visual_embed_size = 512\n",
        "hidden_size = 512\n",
        "\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, visual_embed_size)\n",
        "\n",
        "# Load the trained weights.\n",
        "decoder_file = \"A3DS/pretrained_decoder_3dshapes.pkl\"\n",
        "decoder.load_state_dict(torch.load(decoder_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gl5Gx_wM36T",
        "outputId": "18c1fc56-0eba-4bf1-d49d-be622b3dc0a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  4, 31, 38, 28,  2,  8,  9, 27, 11,  8, 12, 13,  4, 45, 14, 15,  6,\n",
            "        16,  1,  3,  3,  3,  3,  3,  3])\n"
          ]
        }
      ],
      "source": [
        "itemID=0\n",
        "image, target_feats, caption_text, numeric_lbl, caption_indx = A3DS_dataset.__getitem__(itemID)\n",
        "print(caption_indx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRfZCXQ6M36U"
      },
      "source": [
        "The current NIC module was actually trained for later use of two pictures (contrastive image captioning).\n",
        "Therefore, we need to input the picture to be described not once, but twice.\n",
        "(This is otherwise completely innocuous for our current purposes of single-picture captioning.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZscJQOyM36V",
        "outputId": "56b7c820-2bb7-4e56-85d6-0107fa4025a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The picture shows a small orange cylinder in the left corner in front of a purple wall on red floor\n"
          ]
        }
      ],
      "source": [
        "target_features = target_feats.reshape(1,len(target_feats))\n",
        "both_images     = torch.cat((target_features.unsqueeze(1), target_features.unsqueeze(1)), dim=1)\n",
        "output, _, _, _ = decoder.sample(both_images, caption_indx.shape[0])\n",
        "\n",
        "def clean_sentence(output):\n",
        "    \"\"\"\n",
        "    Helper function for visualization purposes.\n",
        "    Transforms list of token indices to a sentence.\n",
        "    Also accepts mulit-dim tensors (for batch size > 1).\n",
        "\n",
        "    Args:\n",
        "    ----\n",
        "    output: torch.Tensor(batch_size, sentence_length)\n",
        "        Tensor representing sentences in form of token indices.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    sentence: str\n",
        "        String representing decoded sentences in natural language.\n",
        "    \"\"\"\n",
        "    list_string = []\n",
        "    for idx in output:\n",
        "        for i in idx:\n",
        "            try:\n",
        "                list_string.append(A3DS_dataset.vocab[\"idx2word\"][i.item()])\n",
        "            except ValueError:\n",
        "                for y in i:\n",
        "                    list_string.append(A3DS_dataset.vocab[\"idx2word\"][y.item()])\n",
        "    sentence = ' '.join(list_string) # Convert list of strings to full string\n",
        "    sentence = sentence.capitalize()  # Capitalize the first letter of the first word\n",
        "    # find index of end token for displaying\n",
        "    if \"end\" in sentence:\n",
        "        len_sentence = sentence.split(\" \").index(\"end\")\n",
        "    else:\n",
        "        len_sentence = len(sentence.split(\" \"))\n",
        "    cleaned_sentence = \" \".join(sentence.split()[:len_sentence])\n",
        "    return(cleaned_sentence)\n",
        "\n",
        "print(clean_sentence(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqStxEx_M36W"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 8.1.1: </span></strong>\n",
        ">\n",
        "> 0. [Just for yourself] Try out different images and generate captions for them. Try to get a feeling for how reliable or good they are. Try to figure out what criteria you use when you intuitively judge a caption as good. Think about what &rsquo;goodness&rsquo; of a generated caption means (also in relation to the ground truth in the training set).\n",
        ">\n",
        "> 1. Describe the architecture of the decoder module that is used in in direct comparison to the set up from the paper [Vinyals et al. (2015)](https://arxiv.org/abs/1411.4555). Highlight at least two differences in model architecture between the model used here and that of Vinyals et al. These differences should all be *major* differences, i.e., differences that *could* plausible have a strong impact on the quality of the results. I.o.w., do not mention trivialities.\n",
        ">\n",
        "> 2. Name at least two things that would be important to know for a direct, close reproduction of Vinyals et al. results that are not or only insufficiently described in the paper.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer to Exercise 8.1.1.1\n",
        "\n",
        "The architecture of the decoder module:\n",
        "\n",
        "The decoder is LSTM-based. Its core is a memory cell c which encodes knowlegde of what has been observed up to this step at every step. Then, this cell's behaviour is controlled by 3 gates: the forget gate f controls whether to forget the current cell value, the input gate i controls whether to read the input of the cell value, and the output gate o controls whether to output the new cell value. These gates are layers calculated by sigmoid functions on trained parameters and in the end they are to be multiplicated (product) to the either keep the new trained parameter or make it 0 in each step. \n",
        "\n",
        "During the training, a copy of LSTM memory is created for the image and each setence word and each output of LSTM at one step is fed to the next step (feed-forwarded). Each word is represented as a one-hot encoded vector. The image is mapped by a vision CNN, and the words are mapped by word embeddings. Besides, each word is also represented as a one-hot encoded vector of vocabulary size.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The weights are trained by stochastic gradient descent with fixed learning rate and no momentum. The loss is calculated by the accumulative negative log likelihoods and are minimized by all LSTM parameters, the word embeddings, and the top layer of CNN. \n",
        "\n",
        "Finally, the sentences are generated by Beam search sampling, with a beam size of 20. \n",
        "\n",
        "It aims to maximize p(S|I), the probability of correct description given the image. And the probability of the sentence S is calculated by the joint probability over S0, S1, ..., St-1. The sum of these log probabilities is then optimized by a stochastic gradient descent. This model is modeled by RNN (Recurrent Neural Network), which has fixed-length hidden states and uses a LSTM (Long-Short Term Memory) net.\n",
        "\n",
        "The images are represented by a CNN (Convolutional Neural Network)."
      ],
      "metadata": {
        "id": "Xq4DHYSC9s4M"
      }
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}